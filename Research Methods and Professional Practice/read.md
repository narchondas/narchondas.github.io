# Research Methods and Professional Practice

This is my learning experience from Module 6 of the MSc Artificial Intelligence

<img width="212" height="178" alt="image" src="https://github.com/user-attachments/assets/ba8d89bb-fd41-499e-ac3c-92f05c3557d5" />



Use the following links to navigate to the start of the sections


[1. Learning outcomes and action plan PDP](#learning-outcomes)

[2. Collaborative Discussions](#collaborative-discussions)

[3. Seminar and formative activities](#seminar-and-formative-activities)

[4. Literature review outline](#literature-review-outline)

[5. Literature review](#literature-review)




## Learning outcomes


### In the module Research Methods and Professional Practice I shall:

*Acquire the ability to study and reflect on key principles and methods in research based on the scientific method and relevant to various disciplines.

*Acquire the ability to examine various research strategies and designs as applicable to projects at hand.

*Acquire the ability to develop research competencies, in particular those relating to the collection and analysis of data types to enable a critical design and evaluation of independent research.

*Have the opportunity to take a reflective and independent approach to the learning process.


### On completion of the module Research Methods and Professional Practice, I will be able to:

*Appraise the professional, legal, social, cultural and ethical issues that affect computing professionals.

*Appraise the principles of academic investigation, applying them to a research topic in the applicable computing field.

*Evaluate critically existing literature, research design and methodology for the chosen topic, including data analysis processes.

*Produce and evaluate critically a research proposal for the chosen topic.


<img width="1000" height="700" alt="image" src="https://github.com/user-attachments/assets/bf246f8d-495f-4a6d-beba-33406b2242d7" />



[Back to the top](#research-methods-and-professional-practice)



## Professional Skills matrix and action plan PDP


### Self-assessment

* Very motivated to learn about Research Methods and Professional Practice and advance with my MSc.

* Focused on timely delivery of good quality assignments.

* Good with time management, note taking and studying new material.



### Set goals

* Study and reflect on key principles and methods in research based on the scientific method and relevant to various disciplines.

* Examine various research strategies and designs as applicable to projects at hand.

* Develop research competencies, in particular those relating to the collection and analysis of data types to enable a critical design and evaluation of independent research.

* Regularly update my GitHub.


### Develop strategies

* Study the required and additional reading of each Module Unit.

* Take e-notes and highlight the most important sections.

* Understand the formative activities and test my knowledge with examples.

* Plan in advance for the assignments.

* Make sure there is sufficient time to fine-tune assignments.

* Gather feedback from tutor and peers.


### Gather resources

* Study all the required and additional resources of each Module Unit. 

* Prepare seminar activities.

* Perform all formative activities.

* Seek other explanatory reliable material on internet (websites, videos, classes etc).

* Reach out to the tutor.


### Create timeline

* Study all required and additional material every week.
  
* Plan in advance the assignments.

* Timely deliver the modelling assignment. 

* Timely submit my e-portfolio.


### Track progress and revise

* Regularly monitor if targets are met.

* Reconsider strategy, if needed.

* Monitor the timely delivery of good quality assignments.



<img width="716" height="428" alt="image" src="https://github.com/user-attachments/assets/7378e68e-2a58-486d-8cda-47a42c8e0627" />



[Back to the top](#research-methods-and-professional-practice)



## Collaborative discussions

### Initial, summary posts & response to peers' posts


### Collaborative Discussion 1


**Codes of Ethics and Professional Conduct**


**Initial Post**

The Blocker Plus case study, provided by the Association of Computing Machinery (ACM), illustrates the ethical complexity of deploying machine learning systems in socially sensitive contexts. While the system initially aligned with CIPA’s (Children’s Internet Protection Act) legal obligations, the developers breached key provisions of the BCS (British Computer Society) Code of Conduct. Activist groups exploited Blocker Plus’s feedback mechanism to manipulate the machine learning model, leading it to classify legitimate content on gay and lesbian marriage, vaccination, climate change, and other socially relevant topics as harmful to children. The failure to prevent bias and safeguard against malicious data input constituted a violation of the principles of Public Interest, particularly the duty to avoid discrimination and to promote inclusivity (BCS, 2021).

Furthermore, the insufficient transparency and lack of public disclosure regarding the model’s limitations contravened the requirement for Professional Competence and Integrity, as responsible practitioners must ensure compliance with legislation and uphold honesty in technical reporting. The decision to preserve a corrupted model, rather than mitigate harms, also breached their Duty to Relevant Authority by neglecting due diligence and public accountability.

Ethically, this case study underscores the centrality of the principles of fairness, accountability, and transparency to contemporary AI governance frameworks (Fjeld et al., 2020; Finn and Shilton, 2023). It also resonates with the European Commission’s High-Level Expert Group on AI, which stresses the necessity of human oversight and explainability as safeguards against algorithmic discrimination. As Kluge Corrêa et al. (2023) observe, the transition from voluntary ethics to binding AI regulation reflects a growing recognition that professional responsibility must be matched by enforceable accountability. This case study exemplifies that professionalism in computing extends beyond legal compliance to the moral stewardship of technology’s societal impact.

 
#### References


BCS (2021) Code of Conduct.

Fjeld, J. et al. (2020) Principled Artificial Intelligence. Berkman Klein Center.

Finn, M. and Shilton, K. (2023) ‘Ethics Governance Development: The Menlo Report’, Social Studies of Science, 53(3), pp. 315–340.



**Summary Post**



The Blocker Plus case study, published by the Association of Computing Machinery (ACM), exemplifies the ethical fragility of machine learning systems deployed in socially sensitive contexts. Although the system initially complied with the Children’s Internet Protection Act (CIPA), its developers violated core principles of the BCS (2021) Code of Conduct by failing to prevent bias, safeguard data integrity, and ensure accountability. Activist groups exploited the feedback mechanism to corrupt the model through malicious data injection, leading it to classify legitimate content, such as LGBTQ+ rights, vaccination, and climate change, as harmful. This incident underscores how insufficient resilience against adversarial manipulation and weak validation mechanisms can translate directly into discriminatory outcomes (Gomez et al., 2024).

Beyond technical failure, the case reveals a deeper ethical deficit in AI system design. As Rahwan (2018) argues, ethical resilience demands that systems anticipate manipulation and adapt to evolving social contexts rather than rely on static principles. The divergence between ethics by design and ethics in practice persists, often aggravated by institutional pressures prioritising speed and functionality over responsibility (Morley et al., 2021).

Moreover, the incident highlights the need for participatory governance frameworks that align professional ethics with democratic oversight (Whittaker et al., 2021). Embedding human oversight, transparency, and explainability within AI governance remains essential for sustaining public trust (Fjeld et al., 2020; Ribeiro et al., 2025). Yet, as Robles and Mallinson (2025) observe, fragmented ethical guidelines are inadequate without enforceable accountability structures and proactive organisational practices.

Ultimately, Blocker Plus illustrates that ethical AI governance must extend beyond legal compliance. It requires continuous risk assessment, adversarial testing, and transparent incident response protocols, ensuring that the moral stewardship of technology is embedded not only in codes of conduct but in the very architecture of AI systems.

 

#### References

BCS (2021) Code of Conduct. London: British Computer Society.

Fjeld, J., Achten, N., Hilligoss, H., Nagy, A. and Srikumar, M. (2020) Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI. Cambridge, MA: Berkman Klein Center for Internet & Society.

Gomez, J.F., Shukla, A., Gómez, S. and Ruiz, C. (2024) ‘Algorithmic arbitrariness in content moderation’, Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT '24), Rio de Janeiro, Brazil, 3–6 June. New York: Association for Computing Machinery, pp. 2234–2253. https://doi.org/10.1145/3630106.3659036

Morley J., Elhalal A., Garcia F, Kinsey L., Mökander J., Floridi L. (2021) Ethics as a Service: A Pragmatic Operationalisation of AI Ethics. Minds Mach (Dordr).  https://doi.org/10.1007/s11023-021-09563-w

Rahwan, I. (2018) Society-in-the-loop: programming the algorithmic social contract. Ethics Inf Technol 20, 5–14 (2018). https://doi.org/10.1007/s10676-017-9430-8

Ribeiro, D., Rocha, T., Pinto, G., Cartaxo, B., Amaral, M., Davila, N. and Camargo, A. (2025) ‘Toward effective AI governance: a review of principles’, arXiv preprint, arXiv:2505.23417. https://doi.org/10.48550/arXiv.2505.23417

Robles, P. and Mallinson, D.J. (2025) ‘Advancing AI governance with a unified theoretical framework: a systematic review’, Perspectives on Public Management and Governance, gvaf013, pp. 1–15. https://doi.org/10.1093/ppmgov/gvaf013

Whittaker, M., Crawford, K., Dobbe, R., Fried, G., Kaziunas, E., Mathur, V. and Schwartz, O. (2021) The AI Now Report 2021: Participatory Governance in AI Systems. New York: AI Now Institute.



**Response to my peers' posts**



I fully agree with your analysis that the Accessibility in Software Development case highlights how disregarding ethical principles can lead to exclusion and reputational harm. As you rightly note, failure to comply with the ACM and BCS codes, especially their requirements to promote fairness, non-discrimination, and equal access, undermines the very social purpose of computing. The ethical obligation of inclusiveness extends beyond technical accessibility as it also concerns the broader duty to ensure that digital systems do not marginalise individuals or communities through design choices, biased data, or neglect of diverse needs.

In this respect, inclusiveness represents both a moral imperative and a professional standard. The BCS Code of Conduct (2021) explicitly requires computing professionals to promote equal access to the benefits of IT and to act without discrimination on any grounds. This obligation aligns with the principles of fairness and human well-being articulated in the ACM Code of Ethics (2025) and with the European Commission’s High-Level Expert Group on AI, which emphasises diversity, accountability, and human oversight as safeguards against exclusion.

As I discussed in relation to the Blocker Plus case, ethical lapses can also emerge in algorithmic contexts where models are manipulated to suppress content on sensitive issues such as same-sex marriage or climate change. Such incidents reveal that inclusiveness must be embedded not only in user interfaces but also in data governance, model transparency, and participatory design, ensuring that technology serves all sectors of society equitably.



#### References

ACM (2025) Code of Ethics and Professional Conduct.

BCS (2021) Code of Conduct.

European Commission (2020) Ethics Guidelines for Trustworthy AI.



I fully agree with your analysis of the Medical Implant Risk Analysis case, which illustrates how the company’s failure to anticipate technical and ethical risks represents not merely a design flaw but a breach of professional responsibility. As you rightly observe, the hard-coded values constitute a neglect of due diligence, directly contravening the BCS Code of Conduct (BCS, 2022) requirement to show due regard for public health, safety, and well-being. However, the company’s ethical duties extend beyond technical competence: they encompass a broader corporate responsibility toward employees, customers, and society at large.

From an internal perspective, management has a duty to foster an ethical culture that empowers engineers to report design flaws without fear of reprisal, thereby reinforcing professional integrity and competence (BCS, 2022). Externally, the company must ensure that its customers, especially vulnerable patients, are treated not merely as consumers but as stakeholders whose safety, autonomy, and dignity are paramount. This aligns with Nickel’s (2011) argument that direct computer-patient interfaces demand heightened ethical attention to maintain trust and uphold patient autonomy. Transparent communication about product risks and the inclusion of diverse user perspectives in testing and evaluation are therefore ethical imperatives.

More broadly, inclusiveness must guide the design and deployment of medical technologies. As emphasised by Pasricha (2022), ethical digital medicine design requires embedding accountability and inclusivity throughout the lifecycle of medical IoT systems. By integrating these principles, transparency, inclusiveness, and accountability into their corporate ethos, companies can ensure that technological innovation serves not only commercial objectives but also the collective welfare and trust of society (BCS, 2022).


 
#### References

BCS, The Chartered Institute for IT (2022) BCS Code of Conduct for Members.

Nickel, P.J. (2011) ‘Ethics in e-trust and e-trustworthiness: the case of direct computer-patient interfaces’, Ethics and Information Technology, 13(4), pp. 355–363.

Pasricha, S. (2022) ‘Ethics for Digital Medicine: A Path for Ethical Emerging Medical IoT Design.’  Arxiv. https://doi.org/10.48550/arXiv.2210.12007



Your discussion of the Abusive Workplace Behaviour case powerfully illustrates how breaches of ethical and professional standards extend beyond individual misconduct to systemic organisational failure. I fully concur that both Max’s actions and Jean’s inaction violate fundamental principles of the ACM and BCS Codes of Conduct, undermining professional integrity and the ethical fabric of the workplace (Gotterbarn, Miller and Rogerson, 2018; BCS, 2021). In such cases, ethical responsibility cannot be viewed as hierarchical but collective, shared across all levels of management and technical staff.

A crucial dimension of this case concerns inclusiveness and respect for diversity. The BCS Code (2021, clauses 1b and 2d) explicitly mandates members to value alternative viewpoints and to avoid unfair discrimination. By tolerating gender-based aggression and denying authorship credit, the organisation not only breaches these duties but also erodes the inclusive culture essential for innovation, trust, and well-being (Johnson, 2020). Ethical computing practice, therefore, requires cultivating environments where all professionals, regardless of gender, ethnicity, or orientation, can contribute freely and safely.

Moreover, psychological safety constitutes an ethical prerequisite for professional competence. Employees who fear reprisal or humiliation cannot exercise critical judgment, creativity, or accountability, which are values central to both ACM principle 1.1 and the BCS commitment to public welfare (ACM, n.d.; Gotterbarn, Miller and Rogerson, 2018). The failure to intervene against abuse thus represents not merely managerial negligence but a profound ethical lapse. Promoting inclusiveness and dignity at work must be recognised as an ongoing ethical duty and a cornerstone of professional excellence in computing.


 
#### References

ACM (n.d.) Case Study: Abusive Workplace Behaviour. Available at: https://www.acm.org/code-of-ethics/case-studies/abusive-workplace-behavior (Accessed: 25 October 2025).

BCS (2021) Code of Conduct. British Computer Society. Available at: https://www.bcs.org/membership/become-a-member/bcs-code-of-conduct/ (Accessed: 25 October 2025).

Gotterbarn, D., Miller, K. and Rogerson, S. (2018) ‘ACM Code of Ethics and Professional Conduct’, Communications of the ACM, 61(1), pp. 21–28.

Johnson, D.G. (2020) Computer Ethics. 5th edn. Harlow: Pearson.



<img width="737" height="445" alt="image" src="https://github.com/user-attachments/assets/d4eb2ce3-0933-4bf4-bdbe-a18cf7b89de0" />



### Collaborative Discussion 2


**Accuracy of information**


**Initial Post**


Abi’s dilemma centres on truthfulness, transparency, and the responsible communication of statistical evidence. As Berenson, Levine and Szabat emphasise, ethical statistical practice requires the accurate reporting of all observed results, the clear communication of analytical procedures used, and the avoidance of selective presentation that may mislead stakeholders (Berenson et al., 2019). Altering data values would constitute a direct breach of professional integrity, but selectively reporting only favourable analyses, while technically using correct data, still represents a serious ethical violation because it distorts the integrity of the evidence base.

The findings of Malički et al. (2023) reinforce this position. Their survey demonstrates that researchers, reviewers, and editors overwhelmingly view full transparency in methods, data, and results as essential to research integrity. Selective reporting, especially the omission of negative findings, is recognised as a form of misconduct because it undermines reproducibility and public trust. Their insights strengthen the argument that Abi is obligated to present all legitimate analyses, both favourable and unfavourable, and to explain any methodological divergence that yields conflicting results.

This duty is further supported by the BCS Code of Ethics, which requires professionals to avoid misrepresentation, act with diligence, and uphold the public interest (BCS, 2021). Even if Abi is not entirely responsible for how the manufacturer ultimately uses the findings, his association with the study gives him an indirect ethical responsibility to the broader community. If he reasonably expects selective dissemination, he may request guarantees of full disclosure, document limitations explicitly, or refuse authorship should the results be misrepresented. Such actions align with principles of accountability, transparency, and professional responsibility widely endorsed in contemporary ethical frameworks (Fjeld et al., 2020; Deckard, 2023).

 
#### References

BCS (2021) BCS Code of Ethics. The Chartered Institute for IT.

Berenson, L., Levine, D. and Szabat, K. (2019) Basic Business Statistics: Concepts and Applications. 14th edn. Pearson.

Deckard, R. (2023) ‘What are Ethics in AI’. IBM Institute for Business Value.

Fjeld, J. et al. (2020) ‘Principled Artificial Intelligence’. Berkman Klein Center Research Publication.

Malički, M., Aalbersberg, I.J., Bouter, L., Mulligan, A. and ter Riet, G. (2023) ‘Transparency in conducting and reporting research’, PLOS ONE, doi: 10.1371/journal.pone.0270054



**Response to my peers' posts**


Your analysis appropriately underscores the seriousness of the ethical dilemma Abi faces, and I fully concur that both data fabrication and selective reporting constitute profound violations of research integrity. As you note, presenting only favourable outcomes distorts the evidential basis on which decisions are made. This is consistent with the core principles of ethical statistical practice, which emphasise accuracy, transparency, and the obligation to communicate all observed results without omission (Berenson et al., 2019). Any attempt to suppress unfavourable findings therefore represents a breach of professional responsibility.

The importance of such transparency is further supported by empirical work showing that researchers broadly recognise full disclosure of methods, data, and results as essential to maintaining scientific integrity and reproducibility (Malički et al., 2023). Your point that selectively withholding negative results undermines public trust directly echoes concerns raised about the broader social consequences of distorted research outputs. Given that the product in question may pose health risks, the ethical imperative to communicate complete and unbiased analyses becomes even more pressing.

Moreover, your argument on the necessity of ethical governance frameworks aligns with the broader expectation that professionals act with honesty, diligence, and in the public interest (BCS, 2021). If Abi anticipates that his findings may be selectively disseminated, he is ethically justified in insisting on full disclosure, documenting limitations, or distancing himself from any report that compromises transparency. Such actions are essential to maintaining both scientific credibility and professional integrity.

 
#### References

BCS (2021) BCS Code of Ethics. The Chartered Institute for IT.

Berenson, L., Levine, D. and Szabat, K. (2019) Basic Business Statistics: Concepts and Applications. 14th edn. Pearson.

Malički, M., Aalbersberg, I.J., Bouter, L., Mulligan, A. and ter Riet, G. (2023) ‘Transparency in conducting and reporting research’, PLOS ONE, doi: 10.1371/journal.pone.0270054



Your analysis accurately captures the seriousness of the ethical dilemma Abi faces. As has been extensively argued in the literature, ethical statistical practice requires the comprehensive reporting of all results and the avoidance of analytical manoeuvres that generate misleading or inconsistent conclusions (Berenson et al., 2019; Bouaziz et al., 2022). Even if numerical values remain correct, presenting only favourable outputs constitutes a distortion of the evidential record and undermines the integrity of the research process.

The broader research community also recognises full transparency as indispensable to scientific credibility and reproducibility. Empirical studies show that omissions or selective disclosure in methods and results are widely viewed as forms of misconduct because they compromise public trust and hinder informed decision making (Malički et al., 2023; Salonen, 2024). In this case, the risks extend beyond scientific norms: inaccurate representations of the nutritional quality or safety of a product may have direct implications for consumer health and expose both Abi and the manufacturer to regulatory or legal consequences (Kitchen and Tourky, 2022).

Professional ethical frameworks reinforce these obligations by requiring diligence, honesty, and the prioritisation of the public interest (BCS, 2021; Fjeld et al., 2020; Deckard, 2023). Abi is therefore justified in presenting both positive and negative findings transparently. If the manufacturer insists on selective reporting, withdrawing from the project remains a legitimate means of preserving his professional integrity and avoiding complicity in misrepresentation.


#### References

BCS (2021) BCS Code of Ethics. The Chartered Institute for IT.

Berenson, L., Levine, D. and Szabat, K. (2019) Basic Business Statistics: Concepts and Applications. 14th edn. Pearson.

Bouaziz, G., Brulin, D. and Campo, E., 2022. Technological solutions for social isolation monitoring of the elderly: a survey of selected projects from academia and industry. Sensors, 22(22), p.8802. Available at: https://doi.org/10.3390/s22228802

Deckard, R. (2023) ‘What are Ethics in AI’. IBM Institute for Business Value.

Fjeld, J. et al. (2020) ‘Principled Artificial Intelligence’. Berkman Klein Center Research Publication.

Kitchen, P. J., & Tourky, M. E. (2022). Integrated marketing communications: A global brand-driven approach. Springer Nature. https://books.google.com.pk/books?hl=en&lr=lang_en&id=YJtXEAAAQBAJ&oi=fnd&pg=PR6&dq=Ethical+Concerns+in+Data+Analysis+for+Whizzz+Cereal&ots=Y1qgb9Om1u&sig=6nmnjMM6UW1eytnXXM3rvh1xXEs&redir_esc=y#v=onepage&q&f=false

Malički, M., Aalbersberg, I.J., Bouter, L., Mulligan, A. and ter Riet, G. (2023) ‘Transparency in conducting and reporting research’, PLOS ONE, doi: 10.1371/journal.pone.0270054

Salonen, A.S., 2024. Creator, saviour, garburator:(Re) imagining the human role in the world through a case of food waste. Social Compass, 71(3), pp.425-441. https://doi.org/10.1177/00377686221144400



I fully agree with your assessment that choosing tests specifically because they portray Whizzz more favourably constitutes a form of analytical bias, violating the norms of integrity and objectivity expected in professional statistical work (ASA, 2018). As has been widely recognised, misleading stakeholders through selective testing can be as ethically damaging as fabrication itself, since it distorts the underlying evidence and undermines confidence in scientific outputs (Gelman and Loken, 2014).

This aligns closely with broader principles of ethical statistical practice, which emphasise complete, transparent reporting of all observed results and the avoidance of selective presentation that may influence decisions in a misleading manner (Berenson et al., 2019). The expectation of full disclosure is further reinforced by empirical evidence showing that researchers overwhelmingly view methodological and results transparency as essential to research integrity (Malički et al., 2023). When consumer health is at stake, the omission of adverse findings becomes even more serious, falling within the broader obligation to prevent harm articulated in contemporary research ethics frameworks (Zwitter, 2014).

I also concur with your view that Abi retains responsibility for the accuracy and clarity of his outputs even if he cannot dictate the manufacturer’s later actions. Professional codes highlight duties of honesty, diligence, and public interest (BCS, 2021; Fjeld et al., 2020; Deckard, 2023). If selective reporting appears likely, providing a balanced, contextualised report, or ultimately withdrawing, may be necessary to preserve both ethical standards and professional integrity.


#### References

American Statistical Association (ASA) (2018) Ethical guidelines for statistical practice. Available at: https://www.amstat.org/your-career/ethical-guidelines-for-statistical-practice (Accessed: 3 December 2025).Berenson, L., Levine, D. and Szabat, K. (2019) Basic Business Statistics: Concepts and Applications. 14th edn. Pearson.

BCS (2021) BCS Code of Ethics. The Chartered Institute for IT.

Berenson, L., Levine, D. and Szabat, K. (2019) Basic Business Statistics: Concepts and Applications. 14th edn. Pearson.

Deckard, R. (2023) ‘What are Ethics in AI’. IBM Institute for Business Value.

Fjeld, J. et al. (2020) ‘Principled Artificial Intelligence’. Berkman Klein Center Research Publication.

Gelman, A. and Loken, E. (2014) ‘The garden of forking paths’, American Scientist, 102(6), pp. 460–465. Available at: https://doi.org/10.1511/2014.111.460

Malički, M., Aalbersberg, I.J., Bouter, L., Mulligan, A. and ter Riet, G. (2023) ‘Transparency in conducting and reporting research’, PLOS ONE, doi: 10.1371/journal.pone.0270054

Zwitter, A. (2014) ‘Big data ethics’, Big Data & Society, 1(2), pp. 1–6. Available at: https://doi.org/10.1177/2053951714559253

<img width="974" height="712" alt="image" src="https://github.com/user-attachments/assets/577abdda-aab9-470e-b437-db28bdc65573" />


**Summary Post**


Abi’s dilemma fundamentally concerns truthfulness, transparency, and responsible statistical communication. Ethical statistical practice requires the accurate reporting of all observed results, clear disclosure of analytical procedures, and the avoidance of selective presentation that may mislead stakeholders (Berenson et al., 2019). While direct data manipulation would constitute an obvious breach of integrity, selectively reporting only favourable analyses remains ethically unacceptable because it distorts the evidential basis on which decisions are made.

Empirical evidence reinforces this obligation. Researchers and gatekeepers (journal editors, peer reviewers, institutional review bodies, funding bodies or sponsors etc.) consistently regard full transparency in methods, data, and results as central to research integrity, with selective omission of unfavourable findings widely recognised as misconduct that undermines reproducibility and public trust (Malički et al., 2023). Moreover, non-financial conflicts of interest, such as the desire to satisfy a client, can bias interpretation and reporting even when data remain unchanged (Montgomery & Belle Weisman, 2021). In industry-sponsored contexts, funding bias may shape not only reporting but the research agenda itself, further heightening the ethical stakes (Bero, 2022).

These concerns align with broader warnings that normalising selective reporting can skew entire research fields over time (Ioannidis, 2005; Fanelli, 2012). Strengthened expectations for data storage, preservation, and methodological clarity can therefore act as safeguards against post-hoc selection and misrepresentation (Wicherts et al., 2011). In line with professional codes and contemporary ethical frameworks, Abi should insist on full disclosure of all results, favourable or otherwise, and consider pre-specified analytical protocols to prevent selective dissemination (BCS, 2021; Fjeld et al., 2020). If transparency cannot be guaranteed, explicitly stating limitations or withdrawing authorship represents a proportionate and ethically defensible response that protects both professional integrity and the public interest (Taquette & Souza, 2022; Gouvea, 2024).


#### References


BCS (2021) BCS Code of Ethics. The Chartered Institute for IT.

Berenson, L., Levine, D. and Szabat, K. (2019) Basic Business Statistics: Concepts and Applications. 14th edn. Pearson.

Fanelli, D. (2012) ‘Negative results are disappearing from most disciplines and countries’, Scientometrics, 90(3), pp. 891–904. Available at: https://doi.org/10.1007/s11192-011-0494-7.

Gouvea, J.S., 2024. Ethical dilemmas in current uses of AI in science education. CBE—Life Sciences Education, 23(1), p.fe3. Available at: https://doi.org/10.1187/cbe.23-12-0239 

Ioannidis, J.P.A. (2005) ‘Why Most Published Research Findings Are False’, PLOS Medicine, 2(8), p. e124. Available at: https://doi.org/10.1371/journal.pmed.0020124

Malički, M., Aalbersberg, I.J., Bouter, L., Mulligan, A. and ter Riet, G. (2023) ‘Transparency in conducting and reporting research’, PLOS ONE, doi: 10.1371/journal.pone.0270054

Montgomery, P. and Belle Weisman, C. (2021) ‘Non-financial conflict of interest in social intervention trials and systematic reviews: An analysis of the issues with case studies and proposals for management’, Children and Youth Services Review, 120, p. 105642. Available at: https://doi.org/10.1016/j.childyouth.2020.105642

Taquette & Souza (2022) Taquette, S.R. and Borges da Matta Souza, L.M., 2022. Ethical dilemmas in qualitative research: A critical literature review. International Journal of Qualitative Methods, 21, p.16094069221078731. Available at: https://doi.org/10.1177/1609406922107873 

Wicherts, J.M., Bakker, M. and Molenaar, D. (2011) ‘Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results’, PLOS ONE, 6(11), p. e26828. Available at: https://doi.org/10.1371/journal.pone.0026828.



[Back to the top](#research-methods-and-professional-practice)



## Seminar and formative activities 



### Unit Formative Activities


**Unit 1**


**Towards a balanced framework for governance and professional responsibility**

The unprecedented expansion of AI technologies has ushered in what Corrêa et al. (2023) term the “AI ethics boom”, a period marked by a proliferation of guidelines, recommendations, and normative frameworks intended to direct the responsible use of AI. While this global momentum reflects growing awareness of the societal implications of automation and algorithmic decision-making, it simultaneously exposes a pressing challenge: how to maintain ethical integrity and public trust while enabling innovation and competitiveness. The ensuing discussion explores the evolution of AI ethics, the different forms of regulatory and self-regulatory governance, and proposes a variable-geometry model capable of harmonising binding obligations with voluntary principles.

AI technologies are now deeply embedded in social, economic, and political systems, influencing healthcare, education, finance, and governance. Corrêa et al. (2023) identify over 200 ethical frameworks worldwide, revealing a remarkable convergence around eight recurrent principles: fairness and non-discrimination, privacy, accountability, transparency, safety and security, professional responsibility, human control of technology, and promotion of human values. These principles echo the findings of Fjeld et al. (2020), who mapped consensus across diverse institutional settings, confirming fairness, transparency, and accountability as near-universal ethical anchors.

The acceleration of AI development, particularly in generative models and predictive analytics, necessitates that these principles move beyond theoretical ethical ideals and acquire enforceable status. As Dawson (2015) observed regarding research ethics, integrity, honesty, and respect for participants and intellectual property are not optional virtues but professional duties. In AI practice, this translates into ensuring that algorithms do not perpetuate bias, violate privacy, or undermine human autonomy. Deckard (2023) further emphasises that AI ethicists must possess interdisciplinary literacy, that is, combining technical proficiency with philosophical, legal, and sociological insight, to mediate between innovation and societal good.

Historically, the governance of AI began as voluntary self-regulation. Technology firms, research institutions, and professional bodies developed codes of conduct and internal review procedures in the absence of formal legislation. The BCS (2021) Code of Conduct, for instance, underscores the duty of computing professionals to act with integrity, safeguard public interest, and promote inclusion and fairness. Such self-commitments aligned with what Corrêa et al. (2023) classify as soft law or non-binding guidelines, designed to integrate ethical principles into practice without imposing coercive sanctions.

However, voluntary codes alone have proven insufficient to mitigate systemic risks such as algorithmic discrimination, misinformation, and opacity. The Menlo Report examined by Finn and Shilton (2023) offers a paradigm shift: recognising that data produced by or about machines can have direct human impact. This ontological realignment, treating network data as ethically sensitive, bridged the gap between technological research and human subjects protection. Consequently, ethics governance shifted from voluntary moral guidance to a more formalised, institutional process embedded within law and public policy.

Across jurisdictions, different trajectories have emerged in the institutionalisation of AI ethics. The EU has assumed a leadership role through the AI Act, a risk-based regulatory framework designed to ensure that AI systems placed on the market are transparent, explainable, and safe. The Act draws heavily upon the High-Level Expert Group on Artificial Intelligence (European Commission, 2019) and embodies a legally binding model, a hard-law approach, that mandates compliance with ethical principles. Its flexibility lies in its proportionality mechanism: high-risk applications (e.g., biometric surveillance, recruitment) face stringent requirements, while low-risk innovations remain less constrained. Nevertheless, critics argue that excessive regulatory burdens could stifle innovation and deter investment, echoing concerns expressed in Fjeld et al. (2020) about balancing accountability with technological progress.

In contrast, the US seems to have prioritised innovation and market dynamism. Federal policy relies primarily on soft law, voluntary standards and guidance such as the National Institute of Standards and Technology (NIST) AI Risk Management Framework, emphasising transparency and industry self-governance rather than prescriptive regulation. China, meanwhile, presents a more complex case: a hybrid model combining state-centric control with strategic promotion of AI leadership. Ethical oversight is closely intertwined with social governance objectives, raising questions about privacy, autonomy, and freedom (Corrêa et al., 2023).

Beyond national frameworks, international institutions such as UNESCO, the OECD, and the G7 have advanced convergent principles promoting human rights, fairness, and accountability. These efforts signal the gradual emergence of a global ethical infrastructure, yet one still fragmented by divergent political and economic priorities.
Given the diversity of legal traditions and innovation ecosystems, a uniform, binding, and comprehensive global framework remains impractical. A pragmatic solution lies in adopting a variable-geometry model, a multi-layered structure combining binding regulations, non-binding recommendations, and voluntary professional codes. Under such model, legally enforceable instruments would target high-risk applications with significant potential for harm (e.g., medical diagnostics, autonomous vehicles), ensuring transparency, auditability, and accountability. Complementing these, soft law instruments such as OECD recommendations or corporate principles like IBM’s Trust and Transparency Guidelines (n.d.) and SAP’s (n.d.) AI Principles, would encourage ethical alignment in lower-risk contexts while allowing agility in technological experimentation.

The strength of such a framework lies in its adaptability. Ethical principles are inherently dynamic.  As AI evolves, so too must the mechanisms that govern it. Continuous review, stakeholder participation, and cross-sector collaboration are therefore vital to maintain legitimacy and public confidence. As Deckard (2023) notes, effective AI ethicists must also participate in public policy debates, bridging the gap between technical design and democratic accountability.

The EU possesses a unique capacity to shape global standards through market power, through the so-called “Brussels effect.” By making compliance with its ethical and legal standards a prerequisite for access to the Single Market, the EU can indirectly compel external actors, including third-country providers, to adhere to high levels of transparency, safety, and fairness. This extraterritorial influence, already observed in data protection through the General Data Protection Regulation (GDPR), could similarly extend to AI, promoting a race to the top in ethical compliance.

Nevertheless, the EU’s approach must remain sufficiently flexible to accommodate innovation while maintaining vigilance against potential harms. Excessive rigidity could drive technological development outside its jurisdiction, undermining both competitiveness and ethical influence. A balance between innovation-friendly regulation and robust enforcement is therefore indispensable.

For computing professionals, the convergence of ethical and legal expectations demands a renewed commitment to professional responsibility. The BCS (2021) and the Menlo framework emphasise the dual obligation to comply with legal norms and to act in the public interest, even when the law remains silent. This entails developing AI systems that are explainable, repeatable, and accountable, qualities that reinforce public trust and reduce systemic risks. Ethical literacy must thus become a core competence of the digital profession, alongside technical proficiency.

In conclusion, the governance of AI should evolve towards a variable-geometry structure in which binding legal norms secure essential safeguards against harm, soft-law instruments promote shared ethical alignment, and professional codes cultivate individual accountability. Such a layered system would ensure that innovation proceeds in a manner consistent with human rights, democratic values, and social welfare. The EU AI Act can serve as a cornerstone for this model, provided it remains adaptive and internationally engaged. Ultimately, the ethical trajectory of AI will depend not solely on laws or algorithms, but on the collective integrity and professional responsibility of those who design, deploy, and oversee intelligent systems.

**References**

BCS (2021) The Chartered Institute for IT: Code of Conduct. London: BCS. 

Corrêa, N.K. et al. (2023) ‘Worldwide AI ethics: A review of 200 guidelines and recommendations for AI governance’, Patterns, 4(10).

Dawson, C. (2015) Projects in Computing and Information Systems: A Student’s Guide. Harlow: Pearson.

Deckard, R. (2023) ‘What Are Ethics in AI’, AI and Ethics Journal, 5(2).

European Commission (2019) High-Level Expert Group on Artificial Intelligence: Ethics Guidelines for Trustworthy AI. Brussels: European Commission.

Finn, M. and Shilton, K. (2023) ‘Ethics governance development: The case of the Menlo Report’, Social Studies of Science, 53(3), pp. 315–340.

Fjeld, J. et al. (2020) Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI. Berkman Klein Center Research Publication.

IBM (no date) Principles for Trust and Transparency. Armonk, NY: IBM.

SAP (no date) Guiding Principles for Artificial Intelligence. Walldorf: SAP.



**Unit 3**


**Source at least 2 papers in a Computing subject of your choice (AI, Cybersecurity, Data Science, or a general interest topic in Computer Science), provided they utilise two different types of research methods to achieve their goal/research aims. Now answer the following questions (please provide justifications for your answers) and be prepared to discuss them in the session: 
-Familiarise yourself with the purpose, problem, objective or research question of each paper. Are they in line with your experience or thoughts on the topic, contributing to the collective body of knowledge in this area?
-Is the research methodology utilised in each paper appropriate for the stated purpose or question? 
-In terms of data collection and analysis, is this also appropriate for the stated purpose or question? 
-Does each paper support its claims and conclusions with explicit arguments or evidence?
-How would you enhance the work/paper?**



**Article: Corrêa N.K., Galvão C., Santos J.W., Del Pino C., Pinto E.P., Barbosa C., Massmann D., Mambrini R., Galvão L., Terem E., de Oliveira N, (2023) Worldwide AI ethics: A review of 200 guidelines and recommendations for AI governance, Patterns, Volume 4, Issue 10, https://doi.org/10.1016/j.patter.2023.100857**

#### Purpose, problem, objective, or research question

The paper aims to determine whether a global consensus exists regarding ethical principles guiding AI development. It addresses the lack of a comprehensive global description of AI ethics guidelines by conducting a meta-analysis of 200 governance policies. This contributes to understanding shared values and gaps in AI governance worldwide.

#### Alignment with field knowledge / contribution to body of knowledge

The study meaningfully contributes to the collective understanding of AI ethics by offering an open-source global dataset and visualisation tool, something previous works lacked. It enhances transparency and accessibility in global AI governance research, aligning with ongoing discourse on responsible AI development.

#### Research methodology appropriateness

The meta-analysis and descriptive research design are appropriate for the objective, as the goal is to review and compare existing guidelines, not test hypotheses. The two-phase method (collection and coding of 200 documents) and typological analysis suit the purpose of mapping global ethical patterns.

#### Data collection and analysis appropriateness

Data were collected from public repositories and web scraping, filtered across five languages, and analysed using manual coding, typologies, and n-gram text mining. These are appropriate for qualitative synthesis and pattern identification. Open-access tools enhance transparency and reproducibility.

#### Evidence and argument support

The authors explicitly support claims with quantitative summaries and comparisons to prior studies. They present clear evidence of geographical, institutional, and thematic patterns to justify their conclusions.

#### Enhancement suggestions

*Expand to non-English and underrepresented regions (Africa, Latin America, Middle East).
    
*Include academic papers beyond guidelines to capture broader ethical discourse.

*Deepen analysis with qualitative interviews or case studies to interpret cultural and contextual differences.

*Address gender bias and language limitations in future iterations.


**Article: Labrou Y. and Finin T.  (1998) Semantics and Conversations for an Agent Communication Language. https://doi.org/10.48550/arXiv.cs/9809034**

#### Purpose, problem, objective, or research question
   
The paper aims to provide a formal semantic foundation for the Knowledge Query and Manipulation Language (KQML), an agent communication language, and to define conversation policies for how agents use KQML performatives to communicate effectively. The central problem addressed is the lack of formal semantics and structured dialogue models in agent communication.

#### Alignment with field knowledge / contribution to body of knowledge

The study contributes significantly to multi-agent systems and AI communication by integrating speech act theory into agent language design. It bridges linguistic theory and software engineering, supporting the development of interoperable intelligent agents, highly aligned with theoretical and practical concerns in AI communication.

#### Research methodology appropriateness

The methodology, conceptual modeling and formal semantic specification, is well-suited for the study’s theoretical aim. The authors employ speech act theory and Definite Clause Grammars (DCGs) to define semantics and conversations, appropriately addressing the abstract nature of communication protocols.

#### Data collection and analysis appropriateness
   
Since this is a theoretical and modeling paper, traditional data collection is not applicable. The analysis consists of formal logic expressions, preconditions/postconditions modeling, and grammar-based simulations. This analytical approach is appropriate for formal language design and validation.

#### Evidence and argument support
   
The paper provides explicit formal definitions, logical frameworks, and illustrative examples. The theoretical constructs are backed by linguistic and computational arguments, ensuring the claims are well-supported.

#### Enhancement suggestions

*Extend evaluation with empirical or simulation-based validation of agent communication efficiency.

*Include comparative analysis with newer agent languages 

*Provide implementation case studies to demonstrate applicability.

*Enhance discussion on scalability and real-world interoperability of KQML semantics.



**Unit 4**


**Inappropriate use of surveys**

**In 2018, Cambridge Analytica was in the news in the United Kingdom and the USA (Confessore, 2018) for obtaining and sharing data obtained from millions of Facebook users. They obtained the data through innocuous surveys on Facebook (you may have seen this type of survey and probably participated at times). This is probably the highest profile of surveys used for alternative means and, probably, monetary gains. However, this happens often through various media.
Consider how exactly this happened and why it was used. Find one or two further examples of inappropriate use of surveys and highlight the impact of all these examples from the various ethical, social, legal and professional standpoints that apply.**



In 2016, Cambridge Analytica executed a large-scale data exploitation operation through the Facebook platform, acquiring personal data from approximately 87 million users without informed consent. The operation utilised a personality quiz application entitled "This Is Your Digital Life," supposedly developed for academic psychological research but repurposed as an instrument for political profiling and targeted campaign messaging. Through Facebook's data-sharing protocols, the application harvested information not only from quiz participants but also from their social network connections, creating an extensive data collection operation that captured users' preferences, behaviours, and psychological characteristics (Cadwalladr & Graham-Harrison, 2018; The New York Times, 2018).

This acquired data enabled the construction of psychographic profiles designed to identify individual voters' psychological predispositions, value orientations, and potential responsiveness to specific messaging strategies. Cambridge Analytica subsequently deployed microtargeted communication campaigns during both the US presidential election and the UK's Brexit referendum, employing data-driven techniques to influence voter decision-making processes through personalised content delivery based on psychological profiling derived from users' own digital footprints.

The proliferation of data-driven persuasion techniques in political campaigns raises significant concerns regarding democratic legitimacy and citizen autonomy. When information provided for allegedly benign purposes is covertly redirected toward political influence operations, the principle of informed consent is fundamentally compromised. Citizens cannot exercise autonomous political judgment when their electoral decisions are influenced by targeted interventions engineered from surveillance of their digital behaviour. This phenomenon transforms electoral processes from deliberative exercises based on public discourse into asymmetric information environments where political actors with sophisticated data analytics capabilities can systematically influence outcomes. The implications extend beyond individual privacy concerns to encompass threats to electoral integrity and the erosion of institutional trust in both digital platforms and democratic processes.

This pattern of data exploitation manifests internationally. In Hungary, government-administered digital infrastructure collecting COVID-19 vaccination data and tax benefit information was subsequently utilised to target citizens with political campaign messaging supporting the incumbent government, thereby demonstrating how state apparatus can be instrumentalised for partisan political purposes (Human Rights Watch, 2022). In Kenya's 2013 and 2017 elections, Cambridge Analytica employed survey instruments presented as academic research to construct voter profiles and deliver psychologically targeted content designed to influence electoral behaviour through appeals to ethnic identity and social divisions (Unwanted Witness, 2020).

Addressing these challenges requires regulatory frameworks that transcend market-based self-governance mechanisms. The application of ethical principles and data protection standards cannot be relegated to market forces or voluntary corporate compliance. Strong national and supra-national regulatory authorities must possess the capacity to impose sanctions proportionate to violating organisations' revenues, thereby eliminating financial incentives for exploitative practices. Effective governance requires rigorous institutional monitoring and supervisory mechanisms capable of detecting violations proactively, public accountability measures that impose reputational costs on non-compliant entities, and systematic public information initiatives to enhance citizen awareness regarding potential data misuse. Safeguarding democratic integrity necessitates not merely regulatory articulation but consistent enforcement mechanisms and institutional accountability for actors who exploit personal data to undermine electoral processes and manipulate citizens through surveillance-enabled psychological targeting.


**References**

Cadwalladr, C. & Graham-Harrison, E. (2018) 'Revealed: 50 million Facebook profiles harvested for Cambridge Analytica in major data breach', The Guardian, 17 March.

The New York Times (2018) 'Cambridge Analytica and Facebook: The Scandal and the Fallout', NYT, 4 April.

Human Rights Watch (2022) 'Hungary: Data Misused for Political Campaigns', Human Rights Watch, 1 December.

Unwanted Witness (2020) 'Data exploitation in digital political campaigns and its implication on electoral democracy', Unwanted Witness, 15 October.




**Unit 7**


**Hypothesis testing and summary measures worksheet**

[Exercise 7.2B](https://docs.google.com/spreadsheets/d/1jk8mYnyx2W1qHmClOpHpf-R2mEWHmi6PWqJslhKHU20/edit?usp=sharing)

[Exercise 7.3D](https://docs.google.com/spreadsheets/d/1s-B2QzU9Jldo3SEO8lJ6lACw4dYrSAw2htWuAdfZChU/edit?usp=sharing)

[Exercise 7.4F](https://docs.google.com/spreadsheets/d/1UAai0d1sHcapUuQ3EykRlCXp1d_0Bs5gYdOSE3uN0Xw/edit?usp=sharing)

[Exercise 7.6B](https://docs.google.com/spreadsheets/d/1AxEK5d7QfPpkHaI3SVntXQAY0g_aAbjlVyavXfZyNFs/edit?usp=sharing)

[Exercise 7.4G](https://docs.google.com/spreadsheets/d/1SrJmuEibpLO6HOnE7T-K0iVmytesBipNb_HBJdeirwc/edit?usp=sharing)

[Exercise 7.1B](https://docs.google.com/spreadsheets/d/1QEj67jGtACV1G-smn2xWvmLdKVijuupEAzjwNIARJDk/edit?usp=sharing)

[Exercise 8.6C](https://docs.google.com/spreadsheets/d/1IS6m74lBdLdUmSV_svzlyQbTVPf2wbYjVG0SfRphULU/edit?usp=sharing)



**Unit 8**



[Exercise 1](https://docs.google.com/spreadsheets/d/1LttPuVb-TASp9j0Bq7lTZZO-M88n7DWtXjdUOEJl6Ds/edit?usp=sharing)

[Exercise 2](https://docs.google.com/spreadsheets/d/1dxhcMBgrcNnfzuDx_8QyFXK6WkoLwyXlqxWtzZQAhTo/edit?usp=sharing)

[Exercise 3](https://docs.google.com/spreadsheets/d/1_aDkxwMv9yb_32Sr_msG0uqiZEbO8hl2BCKlkp8XYZM/edit?usp=sharing)


**Analysis**


<img width="1590" height="427" alt="image" src="https://github.com/user-attachments/assets/1ad24a7b-936b-4e37-9164-9e74b0523ca5" />


<img width="1464" height="417" alt="image" src="https://github.com/user-attachments/assets/88319d59-154a-4a09-ae7b-f3a3e3e0ef9e" />



<img width="600" height="400" alt="image" src="https://github.com/user-attachments/assets/fc27c23c-18ce-43a8-87e0-488ca4dddb3c" />



[Back to the top](#research-methods-and-professional-practice)


## Literature review outline


**The Impact of Large Language Models (LLMs) in the policymaking**

The proposed literature review will focus on the application of Large Language Models (LLMs) within the context of public policymaking and governance. Its academic aim will be to operationalise LLM capabilities for governance contexts, advancing in that way the body of research on AI for governance. The review will seek to examine how LLMs, as an advanced form of Natural Language Processing (NLP), can support policy formulation, analysis, consultation, and evaluation. Its practical aim is to identify models and approaches that may reduce policy analysis time and costs, enhance evidence-based policymaking, and promote transparency and trust within public administrations. The primary audience may comprise researchers in public administration, AI governance, and computational science, as well as policy practitioners interested in the integration of AI tools into governance processes.

There is a need for a systematic review of literature at the intersection of AI, LLMs, and public policymaking, as empirical studies remain fragmented and often technologically driven rather than policy-oriented. The significance of this review lies in bridging the gap between technical capabilities and governance applications. It will contribute to both academic discourse and practical implementation by illustrating how advanced NLP and LLM technologies can facilitate more inclusive, transparent, and efficient decision-making. 

The perspective adopted will be primarily interdisciplinary, integrating insights from computer science, public administration, and policy analysis. To synthesise the literature, a conceptual-analytical framework will be employed, mapping the relationships between LLM capabilities and their technical foundations as well as the governance outcomes. 

Sources will be drawn primarily from peer-reviewed journal articles, academic books, and reputable institutional publications. Preliminary searches have already been conducted through major databases such as Scopus, Google Scholar etc, complemented by grey literature and reports from relevant organisations (e.g., OECD, European Commission, and AI governance institutes). Selection criteria will prioritise recent peer-reviewed studies to ensure up-to-date coverage, while also including foundational works on NLP theory, LLM architectures, and AI ethics to provide theoretical grounding. 

The review will be structured as follows:

1.	Introduction -outlines the purpose, scope, and significance of the literature review.
   
3.	Foundations of NLP and LLMs -presents the technical underpinnings and theoretical models.
   
5.	Capabilities and limitations of NLP Models -evaluates performance, interpretability, and potential ethical considerations.
   
7.	Applications in public sector and tourism -analyses real-world implementations and case studies illustrating cross-sector insights.
   
9.	Conclusion and future directions -summarises findings and proposes pathways for research and policy innovation.
    
Finally, the envisaged literature review will aim to support a balanced and accountable AI integration in governance, aligning technological innovation with democratic principles and public values.

**Indicative of list references (to be further enriched)**

AI.GOV.UK, i.AI Consultation analyser, (n.d.) Available at Consultations - Incubator for Artificial Intelligence - GOV.UK 

Blei, D. (2012), Probabilistic topic models, Communications of the ACM, Volume 55, Issue 4 Pages 77 – 84 https://doi.org/10.1145/2133806.2133826

Blei, D. Ng Michal, A., Jordan I. (2003), Latent Dirichlet Allocation, Journal of Machine Learning Research 3, 993-1022

Buhmann, A., & Fieseler, C. (2021). Towards a deliberative framework for responsible innovation in artificial intelligence. Technology in Society, 64, 101475. https://doi.org/10.1016/j.techsoc.2020.101475

Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K. (2019) ‘BERT: Pre-training of deep bidirectional transformers for language understanding’, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4171–4186.

Fahland, D., Fournier, F., Mosqueira-Rey, E., Hernández-Pereira, E., Alonso-Ríos, D., Rai, A. (2020). Explainable AI: from black box to glass box. Journal of the Academy of Marketing Science, 48(1), 137–141. https://doi.org/10.1007/s11747-019-00710-5

Galati F., Galati R. (2019), Cross-country analysis of perception and emphasis of hotel attributes, Tourism Management, Volume 74, Pages 24-42, https://doi.org/10.1016/j.tourman.2019.02.011.

Guan J., Huang F., Zhao Z., Zhu X. & Huang, M. 2020. A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation. Transactions of the Association for Computational Linguistics, 8:93–108.

Li, Shu & Li, Gang & Law, Rob & Paradies, Yin. (2020). Racism in tourism reviews. Tourism Management. 80. 104100. 10.1016/j.tourman.2020.104100.
 
Ligthart A., Catal C., Tekinerdogan B. (2021) Systematic reviews in sentiment analysis: a tertiary study, Artificial Intelligence Review, 54:4997–5053, https://doi.org/10.1007/s10462-021-09973-3 

Omdena Developing an AI-Driven Sentiment Analysis Tool for Political Actors in El Salvador, Available at Developing an AI-Driven Sentiment Analysis Tool for Political Actors in El Salvador, Omdena
 
Siachos, I., & Karacapilidis, N. (2024). Explainable Artificial Intelligence Methods to Enhance Transparency and Trust in Digital Deliberation Settings. Future Internet, 16(7). https://doi.org/10.3390/fi16070241



<img width="554" height="554" alt="image" src="https://github.com/user-attachments/assets/be5e51ff-71da-4991-99be-2e95bf99851c" />



[Back to the top](#research-methods-and-professional-practice)




## Literature Review


**Introduction**

This literature review examines the applications of Large Language Models (LLMs) within public policymaking and governance, with the academic aim of operationalising LLM capabilities in governance contexts and contributing to the growing body of research on Artificial Intelligence (AI) for public-sector decision-making. As advanced forms of Natural Language Processing (NLP), LLMs offer new possibilities for supporting policy formulation, analysis, consultation, and evaluation through open-text analysis, document classification, legislative interpretation, data mining from reports of international organisations, and generation of predictive and foresight insights. These technologies can assist policymakers and public administration officials in collecting, prioritising, analysing, and retrieving information, as well as summarising complex material and generating substantiated policy options. Crucially, such applications presuppose a human-in-the-loop approach in which technical experts, managers, and organisational hierarchies verify and validate outputs before they are shared with political decision-makers, ensuring both accuracy and accountability.

The practical objective of this review is to identify models and approaches that may reduce the time and cost of policy analysis, strengthen evidence-based policymaking, and promote transparency, inclusiveness, and trust in public administrations. In doing so, LLMs can facilitate open, participatory governance by enabling policymakers to systematically integrate multi-source data, stakeholder opinions and expert assessments into structured insights. Their potential extends to supporting scenario modelling, foresight exercises, and predictive analytics, which can improve the quality and timeliness of policy decisions.

This review is particularly useful because existing empirical studies at the intersection of AI/LLMs/policymaking remain fragmented, often prioritising technical performance over policy relevance. To synthesise the literature, a conceptual-analytical framework is employed to map NLP/LLM capabilities to governance outcomes, providing a structured understanding of both their technical foundations and practical implications.

Sources are drawn primarily from peer-reviewed journal articles, academic books, and reputable institutional reports, supplemented by grey literature and publications from relevant organisations such as the European Commission, OECD and AI governance institutes. Selection criteria prioritise recent studies to ensure up-to-date coverage, while including foundational works on NLP theory, LLM architectures, and AI ethics to provide theoretical grounding. 

**Foundations of NLP and LLMs**

NLP is a foundational area of AI enabling computers to interpret, analyse, and generate human language in ways that are operationally meaningful and actionable. By converting unstructured textual data into structured outputs, NLP facilitates tasks such as classification, information extraction, text summarisation, clustering, and sentiment analysis (Hemmatian and Sohrabi, 2017). In essence, NLP acts as a bridge between human communication and machine-readable formats, providing policymakers and public administration officials with tools to efficiently process and make sense of large volumes of complex textual information. This capability is particularly valuable for evidence-informed governance, where timely and accurate interpretation of legislative texts, stakeholder inputs, reports from international organisations, and policy impact assessments is critical.

More concretely, LLMs represent a sophisticated subset of NLP, built on transformer architectures and trained on massive corpora. They do not replace traditional NLP methods but rather extend and integrate them within a single, scalable framework. By modelling syntax, semantics, and contextual relationships, LLMs provide state-of-the-art capabilities for tasks that were previously computationally intensive or unfeasible at scale. Prominent models such as BERT, RoBERTa, and GPT demonstrate how transformer-based deep learning frameworks can capture nuanced language patterns more effectively than earlier statistical approaches (Devlin et al., 2019). Despite these advances, LLMs remain fundamentally probabilistic systems as they do not “understand” language in a human sense, but infer meaning through vector representations, statistical associations, and optimisation of training objectives (Vinodhini and Chandrasekaran, 2012).

Several NLP techniques underpin policy-relevant applications: 

Text summarisation condenses lengthy and complex legislative texts, reports, stakeholder submissions, or open-ended feedback into concise, informative representations. Extractive summarisation selects key phrases from the original text, whereas abstractive summarisation generates semantically faithful reformulations that mimic human summarisation (Hemmatian and Sohrabi, 2017 in Lighthart et al., 2021). In policymaking, summarisation supports the rapid preparation of briefing notes, impact assessments, and consultation synopses, enabling policymakers to access condensed insights, while preserving the richness of the diverse input.

Sentiment analysis, or opinion mining, detects and categorises evaluative language in text, providing granularity at the document, sentence, or aspect level (Akshi Kumar and Sebastian, 2012; Mite-Baidal et al., 2018). This is particularly important in participatory policymaking, where respondents may express conditional or mixed support for specific measures. Approaches range from lexicon-based systems using dictionaries such as SentiWordNet or VADER, to corpus-based statistical methods, and to supervised machine learning models including Support Vector Machines (SVMs), Long Short-Term Memory networks (LSTMs), and transformer-based architectures such as BERT (Hemmatian and Sohrabi, 2017; Vinodhini and Chandrasekaran, 2012; Mite-Baidal et al., 2018). These methods enable policymakers to systematically capture sentiment across diverse, multilingual contributions, identifying key trends, outliers, and potential areas of contention.

Topic modelling, exemplified by Latent Dirichlet Allocation (LDA), uncovers latent thematic structures within large corpora of text, providing a probabilistic representation of topics and their relationships (Blei et al., 2003). In governance contexts, topic modelling can enable policy teams to cluster contributions by theme, detect emerging issues, and prioritise policy concerns efficiently, enhancing both transparency and inclusiveness in stakeholder engagement processes.

At the technical core of these applications is vectorisation, which converts textual content into numerical representations suitable for machine learning and statistical analysis. Traditional approaches, such as Bag of Words and TF-IDF (Term Frequency-Inverse Document Frequency), capture word frequency and document relevance, while disregarding word order (Salton and McGill, 1983). More advanced embeddings, including Word2Vec and GloVe, capture semantic proximity, while contextual models such as BERT dynamically adjust word representations based on surrounding context, improving performance on downstream tasks such as summarisation, clustering, and classification (Devlin et al., 2019). These capabilities enable scalable and nuanced analysis of multi-format and multilingual policy documents, legislative texts, stakeholder feedback etc.

By integrating these technical functions, NLP and LLMs offer practical utility for public administrations by synthesising large volumes of information, identifying sentiment and thematic trends, and by providing structured insights that inform evidence-based decision-making. When applied thoughtfully, these tools can enhance the operationalisation of frameworks such as the European Commission’s Better Regulation, supporting transparent, inclusive, and participatory policymaking by allowing public administration officials to systematically analyse legislative texts, drafts, reports, consultations and impact assessments. Nonetheless, human oversight remains indispensable, ensuring that outputs are ethically sound, contextually accurate, and aligned with democratic governance principles.

**Capabilities and limitations of NLP Models**

Despite their considerable strengths, NLP and LLM systems face critical limitations that must be acknowledged, particularly in high-stakes environments such as policymaking. Their statistical and probabilistic foundations can lead to misinterpretation of metaphors, idioms, sarcasm, or implied meanings, and can produce “hallucinated” or inaccurate content during generative tasks. Furthermore, models often reflect biases present in their training data, raising ethical concerns and potential distortions in outputs (Akshi Kumar and Sebastian, 2012; Blei, 2012). Even the most advanced architectures lack human-like intentionality, socio-cultural reasoning, and ethical judgement. They infer patterns rather than understanding language, which makes human oversight indispensable, especially when analysing nuanced or sensitive policy matters.

AI-generated outputs, including text summaries or synthetic narratives, can be compelling but present limitations in their semantic comprehension as well. As Hutson (2021) describes, AI language systems are like “a mouth without a brain,” producing text that appears meaningful yet may lack substantive understanding. Bender et al. (2024) similarly caution that humans may erroneously impute meaning to synthetic content, creating risks for misinterpretation of data and open-text input. Other common challenges include repetition, logical inconsistencies, lack of long-range coherence, and limited novelty, all of which highlight the need for careful human validation (Guan et al., 2020; Engstrom et al., 2020). These limitations underscore the importance of maintaining interpretability, traceability, and explainability in any AI-supported policy workflow.

Interpretability and explainability are closely related but distinct concepts crucial for responsible deployment of NLP/LLMs. Interpretability refers to the inherent transparency of a model’s structure, allowing humans to understand its mechanics without additional tools. Simpler models such as decision trees or linear regressions are typically interpretable, as their decision-making process can be directly followed step by step (Barredo Arrieta et al., 2019). Explainability, in contrast, is an active process that clarifies how a model generates specific outputs, particularly in opaque or “black box” systems like deep neural networks. Post-hoc techniques, such as feature importance scores, attention visualisations, textual explanations, and example-based demonstrations, enhance understanding for technical experts, policy officers, and the public (Buhmann & Fieseler, 2021; Coussement et al., 2024; Fahland et al., 2020; Mosqueira-Rey et al., 2020; Siachos & Karacapilidis, 2024). For policy applications, distinguishing between interpretability and explainability is essential.  Policymakers must trust outputs, understand the assumptions behind them, and be able to justify recommendations in democratic decision-making processes.

The operational implications of the NLP capabilities are significant. In policymaking, they can systematically process, among others, unstructured open text of legislation, reports, ministerial decrees, reports of international organisations, stakeholder inputs, position papers, emails, or web submissions, transforming them into structured insights such as topic clusters, sentiment profiles, or summary reports. Within the European Commission, these technologies can enhance, for example, the Better Regulation framework by enabling more efficient synthesis of stakeholders’ contributions from portals like Have Your Say, supporting transparent and evidence-informed policymaking (European Commission, 2021). However, the full value of NLP/LLMs depends not only on human oversight but also on proper institutional integration, and clear accountability frameworks, as explained earlier. Outputs must be explainable and traceable to maintain procedural legitimacy and public trust.

Hybrid model architectures that combine interpretable models with high-performance but opaque LLMs offer a promising approach to balance analytical sophistication with transparency (Barredo Arrieta et al., 2019). Such layered strategies allow cross-validation of outputs, improve reliability, and provide multiple levels of explanation suitable for different stakeholders. By embedding these practices into policy workflows, governments can leverage NLP’s technical advantages while safeguarding ethical standards, transparency, and democratic accountability.

**Applications in the public sector: illustrative examples from the UK and El Salvador**

Real-world adoption of NLP across public administrations demonstrates its ability to enhance efficiency, responsiveness, and democratic accountability. In the United Kingdom, Plymouth City Council, South Hams District Council, and West Devon Borough Council partnered with Commonplace to trial NLP tools for analysing extensive open-text consultation feedback (AI.GOV.UK, i.AI Consultation analyser, n.d.). Manual thematic coding, previously requiring weeks, was replaced with NLP pipelines for keyword extraction, topic modelling, sentiment scoring, and geographic trend mapping. This produced a 66% reduction in processing and validation time, cutting up to 10 minutes of review per submission. Estimated gross savings ranged from £19,000 to £38,000 per consultation, with net savings of £7,000–£30,500 after licensing costs. Consultations with over 3,000 respondents became cost-neutral or cost-saving. Beyond financial effects, NLP enabled earlier detection of emerging issues and sentiment patterns, supporting more agile and legitimate policymaking.

El Salvador similarly illustrates NLP-driven administrative modernisation. Tools were used to classify citizen submissions, flag urgency, and route complaints to relevant ministries through platforms like WhatsApp (Omdena, n.d.). Multilingual models and real-time dashboards revealed sentiment trends and systemic service failures, showing that even resource-constrained governments can use NLP to boost responsiveness and citizen satisfaction (Omdena, n.d.). Together, these cases underscore the need to embed NLP into institutional workflows to reinforce transparency, proportionality, and participatory governance.

**Conclusion and future directions**

The literature reviewed demonstrates that NLP/LLM technologies have significant potential to transform policymaking. When deployed thoughtfully, these tools empower policy officers to engage meaningfully with open text and improve the quality, legitimacy, and responsiveness of public policy decisions. By converting unstructured text, NLP can support the preparation of legislative analyses, consultation synopses, impact assessments, and evaluations. This allows decision-makers to access comprehensive evidence before selecting preferred policy options, strengthening both procedural legitimacy and public trust. In this way, NLP is not merely a technical tool but a foundational enabler of responsive, participatory, and evidence-informed governance in the digital age.

Nevertheless, as already stressed, human oversight remains indispensable, particularly in contexts involving sensitive policy choices, complex societal challenges, or high-stakes decisions. AI systems must function as assistive tools rather than a replacement, augmenting the interpretive capacity of policy units and ensuring, in parallel, outputs are explainable, traceable, and ethically sound. Embedding these AI-enabled tools into policy workflows requires institutional integration, clear accountability frameworks, and user training to safeguard systems are usable, trusted, and transparent.

Further research could explore additional NLP capabilities such as named entity recognition, clustering and classification, transfer learning and multilingual processing, and advanced text generation, as these functions can further enhance the efficiency and inclusivity of policymaking. When integrated with human oversight, such functionalities can help governments address societal, economic, and environmental challenges at scale and in real time.

Overall, this literature review aimed to support a balanced and accountable integration of AI in governance, aligning technological innovation with democratic principles and public values. By strengthening evidence-based policymaking, promoting inclusivity, and supporting transparent, open, and participatory governance, NLP/LLMs have the potential to become strategic enablers for modern liberal democracies confronting complex and “super-wicked” problems. Ensuring that AI supports rather than substitutes human judgment remains key to harnessing these technologies effectively, enhancing transparency and open government, and achieving policy outcomes that are both informed and democratically accountable.


**References**

AI.GOV.UK, i.AI Consultation analyser, (n.d.) Available at Consultations - Incubator for Artificial Intelligence - GOV.UK 

Barredo Arrieta A., Díaz-Rodríguez N., Del Ser J., Bennetot A., Tabik S., Barbado A., Garcia S., Gil-Lopez S., Molina D., Benjamins R., Chatila R., Herrera F., (2020) Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI, Information Fusion, Volume 58, Pages 82-115, https://doi.org/10.1016/j.inffus.2019.12.012

Blei, D. (2012), Probabilistic topic models, Communications of the ACM, Volume 55, Issue 4 Pages 77 – 84 https://doi.org/10.1145/2133806.2133826

Blei, D. Ng Michal, A., Jordan I. (2003), Latent Dirichlet Allocation, Journal of Machine Learning Research 3, 993-1022

Coussement, K., Abedin, M. Z., Kraus, M., Maldonado, S., & Topuz, K. (2024). Explainable AI for enhanced decision-making. Decision Support Systems, 184, 114276. https://doi.org/10.1016/j.dss.2024.114276

Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K. (2019) ‘BERT: Pre-training of deep bidirectional transformers for language understanding’, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4171–4186.

Engstrom, David Freeman and Ho, Daniel E. and Sharkey, C. M. and Cuéllar, M.-F., (2020) Government by Algorithm: Artificial Intelligence in Federal Administrative Agencies. NYU School of Law, Public Law Research Paper No. 20-54, Available at http://dx.doi.org/10.2139/ssrn.3551505

European Commission (2021) Better Regulation Guidelines, SWD(2021) 305 final. Available at: https://commission.europa.eu/document/download/d0bbd77f-bee5-4ee5-b5c4-6110c7605476_en?filename=swd2021_305_en.pdf

European Commission (2021) Better Regulation Toolbox. Available at: https://ec.europa.eu/info/sites/default/files/br_toolbox-nov_2021_en_0.pdf

Hemmatian F, Sohrabi MK (2017) A survey on classification techniques for opinion mining and sentiment analysis. Artif Intell Rev 52(3):1495–1545. https://doi.org/10.1007/s10462-017-9599-6

Hutson, M. (2021) Robo-writers: the rise and risks of language-generating AI, Nature 591, 22-25 doi: https://doi.org/10.1038/d41586-021-00530-0

Kumar A, Sebastian TM (2012) Sentiment analysis: a perspective on its past, present and future. Int J Intell Syst Appl 4(10):1–14. https://doi.org/10.5815/ijisa.2012.10.01

Ligthart A., Catal C., Tekinerdogan B. (2021) Systematic reviews in sentiment analysis: a tertiary study, Artificial Intelligence Review, 54:4997–5053, https://doi.org/10.1007/s10462-021-09973-3 

Mite-Baidal K, Delgado-Vera C, Solís-Avilés E, Espinoza AH, Ortiz-Zambrano J, Varela-Tapia E (2018) Sentiment analysis in education domain: a systematic literature review. Commun Comput Inf Sci 883:285–297. https://doi.org/10.1007/978-3-030-00940-3_21 (Scopus)

Omdena Developing an AI-Driven Sentiment Analysis Tool for Political Actors in El Salvador, Available at Developing an AI-Driven Sentiment Analysis Tool for Political Actors in El Salvador, OmdenaSalton, G. and McGill M. (1983) Introduction to Modern Information Retrieval.

Siachos, I., & Karacapilidis, N. (2024). Explainable Artificial Intelligence Methods to Enhance Transparency and Trust in Digital Deliberation Settings. Future Internet, 16(7). https://doi.org/10.3390/fi16070241

Vinodhini G, Chandrasekaran RM (2012) Sentiment analysis and opinion mining: a survey. Int J 2(6):282–292



<img width="740" height="380" alt="image" src="https://github.com/user-attachments/assets/157ac895-b376-4780-99e7-50b134caa8ec" />



[Back to the top](#research-methods-and-professional-practice)



[Go to main Menu](https://narchondas.github.io/)


