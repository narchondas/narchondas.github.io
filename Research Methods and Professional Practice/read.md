# Research Methods and Professional Practice

This is my learning experience from Module 6 of the MSc Artificial Intelligence

<img width="212" height="178" alt="image" src="https://github.com/user-attachments/assets/ba8d89bb-fd41-499e-ac3c-92f05c3557d5" />



Use the following links to navigate to the start of the sections


[1. Learning outcomes and action plan PDP](#learning-outcomes)

[2. Collaborative Discussions](#collaborative-discussions)

[3. Seminar and formative activities](#seminar-and-formative-activities)

[4. Literature review outline](#literature-review-outline)




## Learning outcomes


### In the module Research Methods and Professional Practice I shall:

*Acquire the ability to study and reflect on key principles and methods in research based on the scientific method and relevant to various disciplines.

*Acquire the ability to examine various research strategies and designs as applicable to projects at hand.

*Acquire the ability to develop research competencies, in particular those relating to the collection and analysis of data types to enable a critical design and evaluation of independent research.

*Have the opportunity to take a reflective and independent approach to the learning process.


### On completion of the module Research Methods and Professional Practice, I will be able to:

*Appraise the professional, legal, social, cultural and ethical issues that affect computing professionals.

*Appraise the principles of academic investigation, applying them to a research topic in the applicable computing field.

*Evaluate critically existing literature, research design and methodology for the chosen topic, including data analysis processes.

*Produce and evaluate critically a research proposal for the chosen topic.


<img width="1000" height="700" alt="image" src="https://github.com/user-attachments/assets/bf246f8d-495f-4a6d-beba-33406b2242d7" />



[Back to the top](#research-methods-and-professional-practice)



## Professional Skills matrix and action plan PDP


### Self-assessment

* Very motivated to learn about Research Methods and Professional Practice and advance with my MSc.

* Focused on timely delivery of good quality assignments.

* Good with time management, note taking and studying new material.



### Set goals

* Study and reflect on key principles and methods in research based on the scientific method and relevant to various disciplines.

* Examine various research strategies and designs as applicable to projects at hand.

* Develop research competencies, in particular those relating to the collection and analysis of data types to enable a critical design and evaluation of independent research.

* Regularly update my GitHub.


### Develop strategies

* Study the required and additional reading of each Module Unit.

* Take e-notes and highlight the most important sections.

* Understand the formative activities and test my knowledge with examples.

* Plan in advance for the assignments.

* Make sure there is sufficient time to fine-tune assignments.

* Gather feedback from tutor and peers.


### Gather resources

* Study all the required and additional resources of each Module Unit. 

* Prepare seminar activities.

* Perform all formative activities.

* Seek other explanatory reliable material on internet (websites, videos, classes etc).

* Reach out to the tutor.


### Create timeline

* Study all required and additional material every week.
  
* Plan in advance the assignments.

* Timely deliver the modelling assignment. 

* Timely submit my e-portfolio.


### Track progress and revise

* Regularly monitor if targets are met.

* Reconsider strategy, if needed.

* Monitor the timely delivery of good quality assignments.



<img width="716" height="428" alt="image" src="https://github.com/user-attachments/assets/7378e68e-2a58-486d-8cda-47a42c8e0627" />



[Back to the top](#research-methods-and-professional-practice)



## Collaborative discussions

### Initial, summary posts & response to peers' posts


### Collaborative Discussion 1


**Codes of Ethics and Professional Conduct**


**Initial Post**

The Blocker Plus case study, provided by the Association of Computing Machinery (ACM), illustrates the ethical complexity of deploying machine learning systems in socially sensitive contexts. While the system initially aligned with CIPA’s (Children’s Internet Protection Act) legal obligations, the developers breached key provisions of the BCS (British Computer Society) Code of Conduct. Activist groups exploited Blocker Plus’s feedback mechanism to manipulate the machine learning model, leading it to classify legitimate content on gay and lesbian marriage, vaccination, climate change, and other socially relevant topics as harmful to children. The failure to prevent bias and safeguard against malicious data input constituted a violation of the principles of Public Interest, particularly the duty to avoid discrimination and to promote inclusivity (BCS, 2021).

Furthermore, the insufficient transparency and lack of public disclosure regarding the model’s limitations contravened the requirement for Professional Competence and Integrity, as responsible practitioners must ensure compliance with legislation and uphold honesty in technical reporting. The decision to preserve a corrupted model, rather than mitigate harms, also breached their Duty to Relevant Authority by neglecting due diligence and public accountability.

Ethically, this case study underscores the centrality of the principles of fairness, accountability, and transparency to contemporary AI governance frameworks (Fjeld et al., 2020; Finn and Shilton, 2023). It also resonates with the European Commission’s High-Level Expert Group on AI, which stresses the necessity of human oversight and explainability as safeguards against algorithmic discrimination. As Kluge Corrêa et al. (2023) observe, the transition from voluntary ethics to binding AI regulation reflects a growing recognition that professional responsibility must be matched by enforceable accountability. This case study exemplifies that professionalism in computing extends beyond legal compliance to the moral stewardship of technology’s societal impact.

 
#### References


BCS (2021) Code of Conduct.

Fjeld, J. et al. (2020) Principled Artificial Intelligence. Berkman Klein Center.

Finn, M. and Shilton, K. (2023) ‘Ethics Governance Development: The Menlo Report’, Social Studies of Science, 53(3), pp. 315–340.



**Summary Post**



The Blocker Plus case study, published by the Association of Computing Machinery (ACM), exemplifies the ethical fragility of machine learning systems deployed in socially sensitive contexts. Although the system initially complied with the Children’s Internet Protection Act (CIPA), its developers violated core principles of the BCS (2021) Code of Conduct by failing to prevent bias, safeguard data integrity, and ensure accountability. Activist groups exploited the feedback mechanism to corrupt the model through malicious data injection, leading it to classify legitimate content, such as LGBTQ+ rights, vaccination, and climate change, as harmful. This incident underscores how insufficient resilience against adversarial manipulation and weak validation mechanisms can translate directly into discriminatory outcomes (Gomez et al., 2024).

Beyond technical failure, the case reveals a deeper ethical deficit in AI system design. As Rahwan (2018) argues, ethical resilience demands that systems anticipate manipulation and adapt to evolving social contexts rather than rely on static principles. The divergence between ethics by design and ethics in practice persists, often aggravated by institutional pressures prioritising speed and functionality over responsibility (Morley et al., 2021).

Moreover, the incident highlights the need for participatory governance frameworks that align professional ethics with democratic oversight (Whittaker et al., 2021). Embedding human oversight, transparency, and explainability within AI governance remains essential for sustaining public trust (Fjeld et al., 2020; Ribeiro et al., 2025). Yet, as Robles and Mallinson (2025) observe, fragmented ethical guidelines are inadequate without enforceable accountability structures and proactive organisational practices.

Ultimately, Blocker Plus illustrates that ethical AI governance must extend beyond legal compliance. It requires continuous risk assessment, adversarial testing, and transparent incident response protocols, ensuring that the moral stewardship of technology is embedded not only in codes of conduct but in the very architecture of AI systems.

 

#### References

BCS (2021) Code of Conduct. London: British Computer Society.

Fjeld, J., Achten, N., Hilligoss, H., Nagy, A. and Srikumar, M. (2020) Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI. Cambridge, MA: Berkman Klein Center for Internet & Society.

Gomez, J.F., Shukla, A., Gómez, S. and Ruiz, C. (2024) ‘Algorithmic arbitrariness in content moderation’, Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT '24), Rio de Janeiro, Brazil, 3–6 June. New York: Association for Computing Machinery, pp. 2234–2253. https://doi.org/10.1145/3630106.3659036

Morley J., Elhalal A., Garcia F, Kinsey L., Mökander J., Floridi L. (2021) Ethics as a Service: A Pragmatic Operationalisation of AI Ethics. Minds Mach (Dordr).  https://doi.org/10.1007/s11023-021-09563-w

Rahwan, I. (2018) Society-in-the-loop: programming the algorithmic social contract. Ethics Inf Technol 20, 5–14 (2018). https://doi.org/10.1007/s10676-017-9430-8

Ribeiro, D., Rocha, T., Pinto, G., Cartaxo, B., Amaral, M., Davila, N. and Camargo, A. (2025) ‘Toward effective AI governance: a review of principles’, arXiv preprint, arXiv:2505.23417. https://doi.org/10.48550/arXiv.2505.23417

Robles, P. and Mallinson, D.J. (2025) ‘Advancing AI governance with a unified theoretical framework: a systematic review’, Perspectives on Public Management and Governance, gvaf013, pp. 1–15. https://doi.org/10.1093/ppmgov/gvaf013

Whittaker, M., Crawford, K., Dobbe, R., Fried, G., Kaziunas, E., Mathur, V. and Schwartz, O. (2021) The AI Now Report 2021: Participatory Governance in AI Systems. New York: AI Now Institute.



**Response to my peers' posts**



I fully agree with your analysis that the Accessibility in Software Development case highlights how disregarding ethical principles can lead to exclusion and reputational harm. As you rightly note, failure to comply with the ACM and BCS codes, especially their requirements to promote fairness, non-discrimination, and equal access, undermines the very social purpose of computing. The ethical obligation of inclusiveness extends beyond technical accessibility as it also concerns the broader duty to ensure that digital systems do not marginalise individuals or communities through design choices, biased data, or neglect of diverse needs.

In this respect, inclusiveness represents both a moral imperative and a professional standard. The BCS Code of Conduct (2021) explicitly requires computing professionals to promote equal access to the benefits of IT and to act without discrimination on any grounds. This obligation aligns with the principles of fairness and human well-being articulated in the ACM Code of Ethics (2025) and with the European Commission’s High-Level Expert Group on AI, which emphasises diversity, accountability, and human oversight as safeguards against exclusion.

As I discussed in relation to the Blocker Plus case, ethical lapses can also emerge in algorithmic contexts where models are manipulated to suppress content on sensitive issues such as same-sex marriage or climate change. Such incidents reveal that inclusiveness must be embedded not only in user interfaces but also in data governance, model transparency, and participatory design, ensuring that technology serves all sectors of society equitably.



#### References

ACM (2025) Code of Ethics and Professional Conduct.

BCS (2021) Code of Conduct.

European Commission (2020) Ethics Guidelines for Trustworthy AI.



I fully agree with your analysis of the Medical Implant Risk Analysis case, which illustrates how the company’s failure to anticipate technical and ethical risks represents not merely a design flaw but a breach of professional responsibility. As you rightly observe, the hard-coded values constitute a neglect of due diligence, directly contravening the BCS Code of Conduct (BCS, 2022) requirement to show due regard for public health, safety, and well-being. However, the company’s ethical duties extend beyond technical competence: they encompass a broader corporate responsibility toward employees, customers, and society at large.

From an internal perspective, management has a duty to foster an ethical culture that empowers engineers to report design flaws without fear of reprisal, thereby reinforcing professional integrity and competence (BCS, 2022). Externally, the company must ensure that its customers, especially vulnerable patients, are treated not merely as consumers but as stakeholders whose safety, autonomy, and dignity are paramount. This aligns with Nickel’s (2011) argument that direct computer-patient interfaces demand heightened ethical attention to maintain trust and uphold patient autonomy. Transparent communication about product risks and the inclusion of diverse user perspectives in testing and evaluation are therefore ethical imperatives.

More broadly, inclusiveness must guide the design and deployment of medical technologies. As emphasised by Pasricha (2022), ethical digital medicine design requires embedding accountability and inclusivity throughout the lifecycle of medical IoT systems. By integrating these principles, transparency, inclusiveness, and accountability into their corporate ethos, companies can ensure that technological innovation serves not only commercial objectives but also the collective welfare and trust of society (BCS, 2022).


 
#### References

BCS, The Chartered Institute for IT (2022) BCS Code of Conduct for Members.

Nickel, P.J. (2011) ‘Ethics in e-trust and e-trustworthiness: the case of direct computer-patient interfaces’, Ethics and Information Technology, 13(4), pp. 355–363.

Pasricha, S. (2022) ‘Ethics for Digital Medicine: A Path for Ethical Emerging Medical IoT Design.’  Arxiv. https://doi.org/10.48550/arXiv.2210.12007



Your discussion of the Abusive Workplace Behaviour case powerfully illustrates how breaches of ethical and professional standards extend beyond individual misconduct to systemic organisational failure. I fully concur that both Max’s actions and Jean’s inaction violate fundamental principles of the ACM and BCS Codes of Conduct, undermining professional integrity and the ethical fabric of the workplace (Gotterbarn, Miller and Rogerson, 2018; BCS, 2021). In such cases, ethical responsibility cannot be viewed as hierarchical but collective, shared across all levels of management and technical staff.

A crucial dimension of this case concerns inclusiveness and respect for diversity. The BCS Code (2021, clauses 1b and 2d) explicitly mandates members to value alternative viewpoints and to avoid unfair discrimination. By tolerating gender-based aggression and denying authorship credit, the organisation not only breaches these duties but also erodes the inclusive culture essential for innovation, trust, and well-being (Johnson, 2020). Ethical computing practice, therefore, requires cultivating environments where all professionals, regardless of gender, ethnicity, or orientation, can contribute freely and safely.

Moreover, psychological safety constitutes an ethical prerequisite for professional competence. Employees who fear reprisal or humiliation cannot exercise critical judgment, creativity, or accountability, which are values central to both ACM principle 1.1 and the BCS commitment to public welfare (ACM, n.d.; Gotterbarn, Miller and Rogerson, 2018). The failure to intervene against abuse thus represents not merely managerial negligence but a profound ethical lapse. Promoting inclusiveness and dignity at work must be recognised as an ongoing ethical duty and a cornerstone of professional excellence in computing.


 
#### References

ACM (n.d.) Case Study: Abusive Workplace Behaviour. Available at: https://www.acm.org/code-of-ethics/case-studies/abusive-workplace-behavior (Accessed: 25 October 2025).

BCS (2021) Code of Conduct. British Computer Society. Available at: https://www.bcs.org/membership/become-a-member/bcs-code-of-conduct/ (Accessed: 25 October 2025).

Gotterbarn, D., Miller, K. and Rogerson, S. (2018) ‘ACM Code of Ethics and Professional Conduct’, Communications of the ACM, 61(1), pp. 21–28.

Johnson, D.G. (2020) Computer Ethics. 5th edn. Harlow: Pearson.



<img width="737" height="445" alt="image" src="https://github.com/user-attachments/assets/d4eb2ce3-0933-4bf4-bdbe-a18cf7b89de0" />



[Back to the top](#research-methods-and-professional-practice)



## Seminar and formative activities 



### Unit Formative Activities


**Unit 1**


**Towards a balanced framework for governance and professional responsibility**

The unprecedented expansion of AI technologies has ushered in what Corrêa et al. (2023) term the “AI ethics boom”, a period marked by a proliferation of guidelines, recommendations, and normative frameworks intended to direct the responsible use of AI. While this global momentum reflects growing awareness of the societal implications of automation and algorithmic decision-making, it simultaneously exposes a pressing challenge: how to maintain ethical integrity and public trust while enabling innovation and competitiveness. The ensuing discussion explores the evolution of AI ethics, the different forms of regulatory and self-regulatory governance, and proposes a variable-geometry model capable of harmonising binding obligations with voluntary principles.

AI technologies are now deeply embedded in social, economic, and political systems, influencing healthcare, education, finance, and governance. Corrêa et al. (2023) identify over 200 ethical frameworks worldwide, revealing a remarkable convergence around eight recurrent principles: fairness and non-discrimination, privacy, accountability, transparency, safety and security, professional responsibility, human control of technology, and promotion of human values. These principles echo the findings of Fjeld et al. (2020), who mapped consensus across diverse institutional settings, confirming fairness, transparency, and accountability as near-universal ethical anchors.

The acceleration of AI development, particularly in generative models and predictive analytics, necessitates that these principles move beyond theoretical ethical ideals and acquire enforceable status. As Dawson (2015) observed regarding research ethics, integrity, honesty, and respect for participants and intellectual property are not optional virtues but professional duties. In AI practice, this translates into ensuring that algorithms do not perpetuate bias, violate privacy, or undermine human autonomy. Deckard (2023) further emphasises that AI ethicists must possess interdisciplinary literacy, that is, combining technical proficiency with philosophical, legal, and sociological insight, to mediate between innovation and societal good.

Historically, the governance of AI began as voluntary self-regulation. Technology firms, research institutions, and professional bodies developed codes of conduct and internal review procedures in the absence of formal legislation. The BCS (2021) Code of Conduct, for instance, underscores the duty of computing professionals to act with integrity, safeguard public interest, and promote inclusion and fairness. Such self-commitments aligned with what Corrêa et al. (2023) classify as soft law or non-binding guidelines, designed to integrate ethical principles into practice without imposing coercive sanctions.

However, voluntary codes alone have proven insufficient to mitigate systemic risks such as algorithmic discrimination, misinformation, and opacity. The Menlo Report examined by Finn and Shilton (2023) offers a paradigm shift: recognising that data produced by or about machines can have direct human impact. This ontological realignment, treating network data as ethically sensitive, bridged the gap between technological research and human subjects protection. Consequently, ethics governance shifted from voluntary moral guidance to a more formalised, institutional process embedded within law and public policy.

Across jurisdictions, different trajectories have emerged in the institutionalisation of AI ethics. The EU has assumed a leadership role through the AI Act, a risk-based regulatory framework designed to ensure that AI systems placed on the market are transparent, explainable, and safe. The Act draws heavily upon the High-Level Expert Group on Artificial Intelligence (European Commission, 2019) and embodies a legally binding model, a hard-law approach, that mandates compliance with ethical principles. Its flexibility lies in its proportionality mechanism: high-risk applications (e.g., biometric surveillance, recruitment) face stringent requirements, while low-risk innovations remain less constrained. Nevertheless, critics argue that excessive regulatory burdens could stifle innovation and deter investment, echoing concerns expressed in Fjeld et al. (2020) about balancing accountability with technological progress.

In contrast, the US seems to have prioritised innovation and market dynamism. Federal policy relies primarily on soft law, voluntary standards and guidance such as the National Institute of Standards and Technology (NIST) AI Risk Management Framework, emphasising transparency and industry self-governance rather than prescriptive regulation. China, meanwhile, presents a more complex case: a hybrid model combining state-centric control with strategic promotion of AI leadership. Ethical oversight is closely intertwined with social governance objectives, raising questions about privacy, autonomy, and freedom (Corrêa et al., 2023).

Beyond national frameworks, international institutions such as UNESCO, the OECD, and the G7 have advanced convergent principles promoting human rights, fairness, and accountability. These efforts signal the gradual emergence of a global ethical infrastructure, yet one still fragmented by divergent political and economic priorities.
Given the diversity of legal traditions and innovation ecosystems, a uniform, binding, and comprehensive global framework remains impractical. A pragmatic solution lies in adopting a variable-geometry model, a multi-layered structure combining binding regulations, non-binding recommendations, and voluntary professional codes. Under such model, legally enforceable instruments would target high-risk applications with significant potential for harm (e.g., medical diagnostics, autonomous vehicles), ensuring transparency, auditability, and accountability. Complementing these, soft law instruments such as OECD recommendations or corporate principles like IBM’s Trust and Transparency Guidelines (n.d.) and SAP’s (n.d.) AI Principles, would encourage ethical alignment in lower-risk contexts while allowing agility in technological experimentation.

The strength of such a framework lies in its adaptability. Ethical principles are inherently dynamic.  As AI evolves, so too must the mechanisms that govern it. Continuous review, stakeholder participation, and cross-sector collaboration are therefore vital to maintain legitimacy and public confidence. As Deckard (2023) notes, effective AI ethicists must also participate in public policy debates, bridging the gap between technical design and democratic accountability.

The EU possesses a unique capacity to shape global standards through market power, through the so-called “Brussels effect.” By making compliance with its ethical and legal standards a prerequisite for access to the Single Market, the EU can indirectly compel external actors, including third-country providers, to adhere to high levels of transparency, safety, and fairness. This extraterritorial influence, already observed in data protection through the General Data Protection Regulation (GDPR), could similarly extend to AI, promoting a race to the top in ethical compliance.

Nevertheless, the EU’s approach must remain sufficiently flexible to accommodate innovation while maintaining vigilance against potential harms. Excessive rigidity could drive technological development outside its jurisdiction, undermining both competitiveness and ethical influence. A balance between innovation-friendly regulation and robust enforcement is therefore indispensable.

For computing professionals, the convergence of ethical and legal expectations demands a renewed commitment to professional responsibility. The BCS (2021) and the Menlo framework emphasise the dual obligation to comply with legal norms and to act in the public interest, even when the law remains silent. This entails developing AI systems that are explainable, repeatable, and accountable, qualities that reinforce public trust and reduce systemic risks. Ethical literacy must thus become a core competence of the digital profession, alongside technical proficiency.

In conclusion, the governance of AI should evolve towards a variable-geometry structure in which binding legal norms secure essential safeguards against harm, soft-law instruments promote shared ethical alignment, and professional codes cultivate individual accountability. Such a layered system would ensure that innovation proceeds in a manner consistent with human rights, democratic values, and social welfare. The EU AI Act can serve as a cornerstone for this model, provided it remains adaptive and internationally engaged. Ultimately, the ethical trajectory of AI will depend not solely on laws or algorithms, but on the collective integrity and professional responsibility of those who design, deploy, and oversee intelligent systems.

**References**

BCS (2021) The Chartered Institute for IT: Code of Conduct. London: BCS. 

Corrêa, N.K. et al. (2023) ‘Worldwide AI ethics: A review of 200 guidelines and recommendations for AI governance’, Patterns, 4(10).

Dawson, C. (2015) Projects in Computing and Information Systems: A Student’s Guide. Harlow: Pearson.

Deckard, R. (2023) ‘What Are Ethics in AI’, AI and Ethics Journal, 5(2).

European Commission (2019) High-Level Expert Group on Artificial Intelligence: Ethics Guidelines for Trustworthy AI. Brussels: European Commission.

Finn, M. and Shilton, K. (2023) ‘Ethics governance development: The case of the Menlo Report’, Social Studies of Science, 53(3), pp. 315–340.

Fjeld, J. et al. (2020) Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI. Berkman Klein Center Research Publication.

IBM (no date) Principles for Trust and Transparency. Armonk, NY: IBM.

SAP (no date) Guiding Principles for Artificial Intelligence. Walldorf: SAP.



**Unit 3**


**Source at least 2 papers in a Computing subject of your choice (AI, Cybersecurity, Data Science, or a general interest topic in Computer Science), provided they utilise two different types of research methods to achieve their goal/research aims. Now answer the following questions (please provide justifications for your answers) and be prepared to discuss them in the session: 
-Familiarise yourself with the purpose, problem, objective or research question of each paper. Are they in line with your experience or thoughts on the topic, contributing to the collective body of knowledge in this area?
-Is the research methodology utilised in each paper appropriate for the stated purpose or question? 
-In terms of data collection and analysis, is this also appropriate for the stated purpose or question? 
-Does each paper support its claims and conclusions with explicit arguments or evidence?
-How would you enhance the work/paper?**



**Article: Corrêa N.K., Galvão C., Santos J.W., Del Pino C., Pinto E.P., Barbosa C., Massmann D., Mambrini R., Galvão L., Terem E., de Oliveira N, (2023) Worldwide AI ethics: A review of 200 guidelines and recommendations for AI governance, Patterns, Volume 4, Issue 10, https://doi.org/10.1016/j.patter.2023.100857**

1. Purpose, problem, objective, or research question

The paper aims to determine whether a global consensus exists regarding ethical principles guiding AI development. It addresses the lack of a comprehensive global description of AI ethics guidelines by conducting a meta-analysis of 200 governance policies. This contributes to understanding shared values and gaps in AI governance worldwide.

2. Alignment with field knowledge / contribution to body of knowledge

The study meaningfully contributes to the collective understanding of AI ethics by offering an open-source global dataset and visualisation tool, something previous works lacked. It enhances transparency and accessibility in global AI governance research, aligning with ongoing discourse on responsible AI development.
   
3. Research methodology appropriateness

The meta-analysis and descriptive research design are appropriate for the objective, as the goal is to review and compare existing guidelines, not test hypotheses. The two-phase method (collection and coding of 200 documents) and typological analysis suit the purpose of mapping global ethical patterns.

4. Data collection and analysis appropriateness

Data were collected from public repositories and web scraping, filtered across five languages, and analysed using manual coding, typologies, and n-gram text mining. These are appropriate for qualitative synthesis and pattern identification. Open-access tools enhance transparency and reproducibility.

5. Evidence and argument support

The authors explicitly support claims with quantitative summaries and comparisons to prior studies. They present clear evidence of geographical, institutional, and thematic patterns to justify their conclusions.

6. Enhancement suggestions

*Expand to non-English and underrepresented regions (Africa, Latin America, Middle East).
    
*Include academic papers beyond guidelines to capture broader ethical discourse.

*Deepen analysis with qualitative interviews or case studies to interpret cultural and contextual differences.

*Address gender bias and language limitations in future iterations.


**Article: Labrou Y. and Finin T.  (1998) Semantics and Conversations for an Agent Communication Language. https://doi.org/10.48550/arXiv.cs/9809034**


1. Purpose, problem, objective, or research question
   
The paper aims to provide a formal semantic foundation for the Knowledge Query and Manipulation Language (KQML), an agent communication language, and to define conversation policies for how agents use KQML performatives to communicate effectively. The central problem addressed is the lack of formal semantics and structured dialogue models in agent communication.

2. Alignment with field knowledge / contribution to body of knowledge

The study contributes significantly to multi-agent systems and AI communication by integrating speech act theory into agent language design. It bridges linguistic theory and software engineering, supporting the development of interoperable intelligent agents, highly aligned with theoretical and practical concerns in AI communication.

3. Research methodology appropriateness

The methodology, conceptual modeling and formal semantic specification, is well-suited for the study’s theoretical aim. The authors employ speech act theory and Definite Clause Grammars (DCGs) to define semantics and conversations, appropriately addressing the abstract nature of communication protocols.
 
4. Data collection and analysis appropriateness
   
Since this is a theoretical and modeling paper, traditional data collection is not applicable. The analysis consists of formal logic expressions, preconditions/postconditions modeling, and grammar-based simulations. This analytical approach is appropriate for formal language design and validation.

5. Evidence and argument support
   
The paper provides explicit formal definitions, logical frameworks, and illustrative examples. The theoretical constructs are backed by linguistic and computational arguments, ensuring the claims are well-supported.

6. Enhancement suggestions

*Extend evaluation with empirical or simulation-based validation of agent communication efficiency.

*Include comparative analysis with newer agent languages 

*Provide implementation case studies to demonstrate applicability.

*Enhance discussion on scalability and real-world interoperability of KQML semantics.



**Unit 4**


**Inappropriate use of surveys**

**In 2018, Cambridge Analytica was in the news in the United Kingdom and the USA (Confessore, 2018) for obtaining and sharing data obtained from millions of Facebook users. They obtained the data through innocuous surveys on Facebook (you may have seen this type of survey and probably participated at times). This is probably the highest profile of surveys used for alternative means and, probably, monetary gains. However, this happens often through various media.
Consider how exactly this happened and why it was used. Find one or two further examples of inappropriate use of surveys and highlight the impact of all these examples from the various ethical, social, legal and professional standpoints that apply.**



In 2016, Cambridge Analytica executed a large-scale data exploitation operation through the Facebook platform, acquiring personal data from approximately 87 million users without informed consent. The operation utilised a personality quiz application entitled "This Is Your Digital Life," supposedly developed for academic psychological research but repurposed as an instrument for political profiling and targeted campaign messaging. Through Facebook's data-sharing protocols, the application harvested information not only from quiz participants but also from their social network connections, creating an extensive data collection operation that captured users' preferences, behaviours, and psychological characteristics (Cadwalladr & Graham-Harrison, 2018; The New York Times, 2018).

This acquired data enabled the construction of psychographic profiles designed to identify individual voters' psychological predispositions, value orientations, and potential responsiveness to specific messaging strategies. Cambridge Analytica subsequently deployed microtargeted communication campaigns during both the US presidential election and the UK's Brexit referendum, employing data-driven techniques to influence voter decision-making processes through personalised content delivery based on psychological profiling derived from users' own digital footprints.

The proliferation of data-driven persuasion techniques in political campaigns raises significant concerns regarding democratic legitimacy and citizen autonomy. When information provided for allegedly benign purposes is covertly redirected toward political influence operations, the principle of informed consent is fundamentally compromised. Citizens cannot exercise autonomous political judgment when their electoral decisions are influenced by targeted interventions engineered from surveillance of their digital behaviour. This phenomenon transforms electoral processes from deliberative exercises based on public discourse into asymmetric information environments where political actors with sophisticated data analytics capabilities can systematically influence outcomes. The implications extend beyond individual privacy concerns to encompass threats to electoral integrity and the erosion of institutional trust in both digital platforms and democratic processes.

This pattern of data exploitation manifests internationally. In Hungary, government-administered digital infrastructure collecting COVID-19 vaccination data and tax benefit information was subsequently utilised to target citizens with political campaign messaging supporting the incumbent government, thereby demonstrating how state apparatus can be instrumentalised for partisan political purposes (Human Rights Watch, 2022). In Kenya's 2013 and 2017 elections, Cambridge Analytica employed survey instruments presented as academic research to construct voter profiles and deliver psychologically targeted content designed to influence electoral behaviour through appeals to ethnic identity and social divisions (Unwanted Witness, 2020).

Addressing these challenges requires regulatory frameworks that transcend market-based self-governance mechanisms. The application of ethical principles and data protection standards cannot be relegated to market forces or voluntary corporate compliance. Strong national and supra-national regulatory authorities must possess the capacity to impose sanctions proportionate to violating organisations' revenues, thereby eliminating financial incentives for exploitative practices. Effective governance requires rigorous institutional monitoring and supervisory mechanisms capable of detecting violations proactively, public accountability measures that impose reputational costs on non-compliant entities, and systematic public information initiatives to enhance citizen awareness regarding potential data misuse. Safeguarding democratic integrity necessitates not merely regulatory articulation but consistent enforcement mechanisms and institutional accountability for actors who exploit personal data to undermine electoral processes and manipulate citizens through surveillance-enabled psychological targeting.


**References**

Cadwalladr, C. & Graham-Harrison, E. (2018) 'Revealed: 50 million Facebook profiles harvested for Cambridge Analytica in major data breach', The Guardian, 17 March.

The New York Times (2018) 'Cambridge Analytica and Facebook: The Scandal and the Fallout', NYT, 4 April.

Human Rights Watch (2022) 'Hungary: Data Misused for Political Campaigns', Human Rights Watch, 1 December.

Unwanted Witness (2020) 'Data exploitation in digital political campaigns and its implication on electoral democracy', Unwanted Witness, 15 October.





<img width="600" height="400" alt="image" src="https://github.com/user-attachments/assets/fc27c23c-18ce-43a8-87e0-488ca4dddb3c" />



[Back to the top](#research-methods-and-professional-practice)


## Literature review outline


**The Impact of Large Language Models (LLMs) in the policymaking**

The proposed literature review will focus on the application of Large Language Models (LLMs) within the context of public policymaking and governance. Its academic aim will be to operationalise LLM capabilities for governance contexts, advancing in that way the body of research on AI for governance. The review will seek to examine how LLMs, as an advanced form of Natural Language Processing (NLP), can support policy formulation, analysis, consultation, and evaluation. Its practical aim is to identify models and approaches that may reduce policy analysis time and costs, enhance evidence-based policymaking, and promote transparency and trust within public administrations. The primary audience may comprise researchers in public administration, AI governance, and computational science, as well as policy practitioners interested in the integration of AI tools into governance processes.

There is a need for a systematic review of literature at the intersection of AI, LLMs, and public policymaking, as empirical studies remain fragmented and often technologically driven rather than policy-oriented. The significance of this review lies in bridging the gap between technical capabilities and governance applications. It will contribute to both academic discourse and practical implementation by illustrating how advanced NLP and LLM technologies can facilitate more inclusive, transparent, and efficient decision-making. 

The perspective adopted will be primarily interdisciplinary, integrating insights from computer science, public administration, and policy analysis. To synthesise the literature, a conceptual-analytical framework will be employed, mapping the relationships between LLM capabilities and their technical foundations as well as the governance outcomes. 

Sources will be drawn primarily from peer-reviewed journal articles, academic books, and reputable institutional publications. Preliminary searches have already been conducted through major databases such as Scopus, Google Scholar etc, complemented by grey literature and reports from relevant organisations (e.g., OECD, European Commission, and AI governance institutes). Selection criteria will prioritise recent peer-reviewed studies to ensure up-to-date coverage, while also including foundational works on NLP theory, LLM architectures, and AI ethics to provide theoretical grounding. 

The review will be structured as follows:

1.	Introduction -outlines the purpose, scope, and significance of the literature review.
   
3.	Foundations of NLP and LLMs -presents the technical underpinnings and theoretical models.
   
5.	Capabilities and limitations of NLP Models -evaluates performance, interpretability, and potential ethical considerations.
   
7.	Applications in public sector and tourism -analyses real-world implementations and case studies illustrating cross-sector insights.
   
9.	Conclusion and future directions -summarises findings and proposes pathways for research and policy innovation.
    
Finally, the envisaged literature review will aim to support a balanced and accountable AI integration in governance, aligning technological innovation with democratic principles and public values.

**Indicative of list references (to be further enriched)**

AI.GOV.UK, i.AI Consultation analyser, (n.d.) Available at Consultations - Incubator for Artificial Intelligence - GOV.UK 

Blei, D. (2012), Probabilistic topic models, Communications of the ACM, Volume 55, Issue 4 Pages 77 – 84 https://doi.org/10.1145/2133806.2133826

Blei, D. Ng Michal, A., Jordan I. (2003), Latent Dirichlet Allocation, Journal of Machine Learning Research 3, 993-1022

Buhmann, A., & Fieseler, C. (2021). Towards a deliberative framework for responsible innovation in artificial intelligence. Technology in Society, 64, 101475. https://doi.org/10.1016/j.techsoc.2020.101475

Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K. (2019) ‘BERT: Pre-training of deep bidirectional transformers for language understanding’, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4171–4186.

Fahland, D., Fournier, F., Mosqueira-Rey, E., Hernández-Pereira, E., Alonso-Ríos, D., Rai, A. (2020). Explainable AI: from black box to glass box. Journal of the Academy of Marketing Science, 48(1), 137–141. https://doi.org/10.1007/s11747-019-00710-5

Galati F., Galati R. (2019), Cross-country analysis of perception and emphasis of hotel attributes, Tourism Management, Volume 74, Pages 24-42, https://doi.org/10.1016/j.tourman.2019.02.011.

Guan J., Huang F., Zhao Z., Zhu X. & Huang, M. 2020. A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation. Transactions of the Association for Computational Linguistics, 8:93–108.

Li, Shu & Li, Gang & Law, Rob & Paradies, Yin. (2020). Racism in tourism reviews. Tourism Management. 80. 104100. 10.1016/j.tourman.2020.104100.
 Ligthart A., Catal C., Tekinerdogan B. (2021) Systematic reviews in sentiment analysis: a tertiary study, Artificial Intelligence Review, 54:4997–5053, https://doi.org/10.1007/s10462-021-09973-3 
 
Omdena Developing an AI-Driven Sentiment Analysis Tool for Political Actors in El Salvador, Available at Developing an AI-Driven Sentiment Analysis Tool for Political Actors in El Salvador - Omdena | Projects | Omdena

Siachos, I., & Karacapilidis, N. (2024). Explainable Artificial Intelligence Methods to Enhance Transparency and Trust in Digital Deliberation Settings. Future Internet, 16(7). https://doi.org/10.3390/fi16070241



<img width="554" height="554" alt="image" src="https://github.com/user-attachments/assets/be5e51ff-71da-4991-99be-2e95bf99851c" />



[Back to the top](#research-methods-and-professional-practice)



[Go to main Menu](https://narchondas.github.io/)


