# Research Methods and Professional Practice

This is my learning experience from Module 6 of the MSc Artificial Intelligence

<img width="212" height="178" alt="image" src="https://github.com/user-attachments/assets/ba8d89bb-fd41-499e-ac3c-92f05c3557d5" />



Use the following links to navigate to the start of the sections


[1. Learning outcomes and action plan PDP](#learning-outcomes)

[2. Collaborative Discussions](#collaborative-discussions)

[3. Seminar and formative activities](#seminar-and-formative-activities)




## Learning outcomes


### In the module Research Methods and Professional Practice I shall:

*Acquire the ability to study and reflect on key principles and methods in research based on the scientific method and relevant to various disciplines.

*Acquire the ability to examine various research strategies and designs as applicable to projects at hand.

*Acquire the ability to develop research competencies, in particular those relating to the collection and analysis of data types to enable a critical design and evaluation of independent research.

*Have the opportunity to take a reflective and independent approach to the learning process.


### On completion of the module Research Methods and Professional Practice, I will be able to:

*Appraise the professional, legal, social, cultural and ethical issues that affect computing professionals.

*Appraise the principles of academic investigation, applying them to a research topic in the applicable computing field.

*Evaluate critically existing literature, research design and methodology for the chosen topic, including data analysis processes.

*Produce and evaluate critically a research proposal for the chosen topic.


<img width="1000" height="700" alt="image" src="https://github.com/user-attachments/assets/bf246f8d-495f-4a6d-beba-33406b2242d7" />



[Back to the top](#research-methods-and-professional-practice)



## Professional Skills matrix and action plan PDP


### Self-assessment

* Very motivated to learn about Research Methods and Professional Practice and advance with my MSc.

* Focused on timely delivery of good quality assignments.

* Good with time management, note taking and studying new material.



### Set goals

* Study and reflect on key principles and methods in research based on the scientific method and relevant to various disciplines.

* Examine various research strategies and designs as applicable to projects at hand.

* Develop research competencies, in particular those relating to the collection and analysis of data types to enable a critical design and evaluation of independent research.

* Regularly update my GitHub.


### Develop strategies

* Study the required and additional reading of each Module Unit.

* Take e-notes and highlight the most important sections.

* Understand the formative activities and test my knowledge with examples.

* Plan in advance for the assignments.

* Make sure there is sufficient time to fine-tune assignments.

* Gather feedback from tutor and peers.


### Gather resources

* Study all the required and additional resources of each Module Unit. 

* Prepare seminar activities.

* Perform all formative activities.

* Seek other explanatory reliable material on internet (websites, videos, classes etc).

* Reach out to the tutor.


### Create timeline

* Study all required and additional material every week.
  
* Plan in advance the assignments.

* Timely deliver the modelling assignment. 

* Timely submit my e-portfolio.


### Track progress and revise

* Regularly monitor if targets are met.

* Reconsider strategy, if needed.

* Monitor the timely delivery of good quality assignments.



<img width="716" height="428" alt="image" src="https://github.com/user-attachments/assets/7378e68e-2a58-486d-8cda-47a42c8e0627" />



[Back to the top](#research-methods-and-professional-practice)



## Collaborative discussions

### Initial, summary posts & response to peers' posts


### Collaborative Discussion 1


**Codes of Ethics and Professional Conduct**


**Initial Post**

The Blocker Plus case study, provided by the Association of Computing Machinery (ACM), illustrates the ethical complexity of deploying machine learning systems in socially sensitive contexts. While the system initially aligned with CIPA’s (Children’s Internet Protection Act) legal obligations, the developers breached key provisions of the BCS (British Computer Society) Code of Conduct. Activist groups exploited Blocker Plus’s feedback mechanism to manipulate the machine learning model, leading it to classify legitimate content on gay and lesbian marriage, vaccination, climate change, and other socially relevant topics as harmful to children. The failure to prevent bias and safeguard against malicious data input constituted a violation of the principles of Public Interest, particularly the duty to avoid discrimination and to promote inclusivity (BCS, 2021).

Furthermore, the insufficient transparency and lack of public disclosure regarding the model’s limitations contravened the requirement for Professional Competence and Integrity, as responsible practitioners must ensure compliance with legislation and uphold honesty in technical reporting. The decision to preserve a corrupted model, rather than mitigate harms, also breached their Duty to Relevant Authority by neglecting due diligence and public accountability.

Ethically, this case study underscores the centrality of the principles of fairness, accountability, and transparency to contemporary AI governance frameworks (Fjeld et al., 2020; Finn and Shilton, 2023). It also resonates with the European Commission’s High-Level Expert Group on AI, which stresses the necessity of human oversight and explainability as safeguards against algorithmic discrimination. As Kluge Corrêa et al. (2023) observe, the transition from voluntary ethics to binding AI regulation reflects a growing recognition that professional responsibility must be matched by enforceable accountability. This case study exemplifies that professionalism in computing extends beyond legal compliance to the moral stewardship of technology’s societal impact.

 
#### References


BCS (2021) Code of Conduct.

Fjeld, J. et al. (2020) Principled Artificial Intelligence. Berkman Klein Center.

Finn, M. and Shilton, K. (2023) ‘Ethics Governance Development: The Menlo Report’, Social Studies of Science, 53(3), pp. 315–340.



<img width="737" height="445" alt="image" src="https://github.com/user-attachments/assets/d4eb2ce3-0933-4bf4-bdbe-a18cf7b89de0" />



[Back to the top](#research-methods-and-professional-practice)



## Seminar and formative activities 



### Unit Formative Activities


**Reflective Activity 1 – Ethics in Computing in the age of Generative AI**


**Towards a Balanced Framework for Governance and Professional Responsibility**

The unprecedented expansion of AI technologies has ushered in what Corrêa et al. (2023) term the “AI ethics boom”, a period marked by a proliferation of guidelines, recommendations, and normative frameworks intended to direct the responsible use of AI. While this global momentum reflects growing awareness of the societal implications of automation and algorithmic decision-making, it simultaneously exposes a pressing challenge: how to maintain ethical integrity and public trust while enabling innovation and competitiveness. The ensuing discussion explores the evolution of AI ethics, the different forms of regulatory and self-regulatory governance, and proposes a variable-geometry model capable of harmonising binding obligations with voluntary principles.

AI technologies are now deeply embedded in social, economic, and political systems, influencing healthcare, education, finance, and governance. Corrêa et al. (2023) identify over 200 ethical frameworks worldwide, revealing a remarkable convergence around eight recurrent principles: fairness and non-discrimination, privacy, accountability, transparency, safety and security, professional responsibility, human control of technology, and promotion of human values. These principles echo the findings of Fjeld et al. (2020), who mapped consensus across diverse institutional settings, confirming fairness, transparency, and accountability as near-universal ethical anchors.

The acceleration of AI development, particularly in generative models and predictive analytics, necessitates that these principles move beyond theoretical ethical ideals and acquire enforceable status. As Dawson (2015) observed regarding research ethics, integrity, honesty, and respect for participants and intellectual property are not optional virtues but professional duties. In AI practice, this translates into ensuring that algorithms do not perpetuate bias, violate privacy, or undermine human autonomy. Deckard (2023) further emphasises that AI ethicists must possess interdisciplinary literacy, that is, combining technical proficiency with philosophical, legal, and sociological insight, to mediate between innovation and societal good.

Historically, the governance of AI began as voluntary self-regulation. Technology firms, research institutions, and professional bodies developed codes of conduct and internal review procedures in the absence of formal legislation. The BCS (2021) Code of Conduct, for instance, underscores the duty of computing professionals to act with integrity, safeguard public interest, and promote inclusion and fairness. Such self-commitments aligned with what Corrêa et al. (2023) classify as soft law or non-binding guidelines, designed to integrate ethical principles into practice without imposing coercive sanctions.

However, voluntary codes alone have proven insufficient to mitigate systemic risks such as algorithmic discrimination, misinformation, and opacity. The Menlo Report examined by Finn and Shilton (2023) offers a paradigm shift: recognising that data produced by or about machines can have direct human impact. This ontological realignment, treating network data as ethically sensitive, bridged the gap between technological research and human subjects protection. Consequently, ethics governance shifted from voluntary moral guidance to a more formalised, institutional process embedded within law and public policy.

Across jurisdictions, different trajectories have emerged in the institutionalisation of AI ethics. The EU has assumed a leadership role through the AI Act, a risk-based regulatory framework designed to ensure that AI systems placed on the market are transparent, explainable, and safe. The Act draws heavily upon the High-Level Expert Group on Artificial Intelligence (European Commission, 2019) and embodies a legally binding model, a hard-law approach, that mandates compliance with ethical principles. Its flexibility lies in its proportionality mechanism: high-risk applications (e.g., biometric surveillance, recruitment) face stringent requirements, while low-risk innovations remain less constrained. Nevertheless, critics argue that excessive regulatory burdens could stifle innovation and deter investment, echoing concerns expressed in Fjeld et al. (2020) about balancing accountability with technological progress.

In contrast, the US seems to have prioritised innovation and market dynamism. Federal policy relies primarily on soft law, voluntary standards and guidance such as the National Institute of Standards and Technology (NIST) AI Risk Management Framework, emphasising transparency and industry self-governance rather than prescriptive regulation. China, meanwhile, presents a more complex case: a hybrid model combining state-centric control with strategic promotion of AI leadership. Ethical oversight is closely intertwined with social governance objectives, raising questions about privacy, autonomy, and freedom (Corrêa et al., 2023).

Beyond national frameworks, international institutions such as UNESCO, the OECD, and the G7 have advanced convergent principles promoting human rights, fairness, and accountability. These efforts signal the gradual emergence of a global ethical infrastructure, yet one still fragmented by divergent political and economic priorities.
Given the diversity of legal traditions and innovation ecosystems, a uniform, binding, and comprehensive global framework remains impractical. A pragmatic solution lies in adopting a variable-geometry model, a multi-layered structure combining binding regulations, non-binding recommendations, and voluntary professional codes. Under such model, legally enforceable instruments would target high-risk applications with significant potential for harm (e.g., medical diagnostics, autonomous vehicles), ensuring transparency, auditability, and accountability. Complementing these, soft law instruments such as OECD recommendations or corporate principles like IBM’s Trust and Transparency Guidelines (n.d.) and SAP’s (n.d.) AI Principles, would encourage ethical alignment in lower-risk contexts while allowing agility in technological experimentation.

The strength of such a framework lies in its adaptability. Ethical principles are inherently dynamic.  As AI evolves, so too must the mechanisms that govern it. Continuous review, stakeholder participation, and cross-sector collaboration are therefore vital to maintain legitimacy and public confidence. As Deckard (2023) notes, effective AI ethicists must also participate in public policy debates, bridging the gap between technical design and democratic accountability.

The EU possesses a unique capacity to shape global standards through market power, through the so-called “Brussels effect.” By making compliance with its ethical and legal standards a prerequisite for access to the Single Market, the EU can indirectly compel external actors, including third-country providers, to adhere to high levels of transparency, safety, and fairness. This extraterritorial influence, already observed in data protection through the General Data Protection Regulation (GDPR), could similarly extend to AI, promoting a race to the top in ethical compliance.

Nevertheless, the EU’s approach must remain sufficiently flexible to accommodate innovation while maintaining vigilance against potential harms. Excessive rigidity could drive technological development outside its jurisdiction, undermining both competitiveness and ethical influence. A balance between innovation-friendly regulation and robust enforcement is therefore indispensable.

For computing professionals, the convergence of ethical and legal expectations demands a renewed commitment to professional responsibility. The BCS (2021) and the Menlo framework emphasise the dual obligation to comply with legal norms and to act in the public interest, even when the law remains silent. This entails developing AI systems that are explainable, repeatable, and accountable, qualities that reinforce public trust and reduce systemic risks. Ethical literacy must thus become a core competence of the digital profession, alongside technical proficiency.

In conclusion, the governance of AI should evolve towards a variable-geometry structure in which binding legal norms secure essential safeguards against harm, soft-law instruments promote shared ethical alignment, and professional codes cultivate individual accountability. Such a layered system would ensure that innovation proceeds in a manner consistent with human rights, democratic values, and social welfare. The EU AI Act can serve as a cornerstone for this model, provided it remains adaptive and internationally engaged. Ultimately, the ethical trajectory of AI will depend not solely on laws or algorithms, but on the collective integrity and professional responsibility of those who design, deploy, and oversee intelligent systems.

**References**

BCS (2021) The Chartered Institute for IT: Code of Conduct. London: BCS. 

Corrêa, N.K. et al. (2023) ‘Worldwide AI ethics: A review of 200 guidelines and recommendations for AI governance’, Patterns, 4(10).

Dawson, C. (2015) Projects in Computing and Information Systems: A Student’s Guide. Harlow: Pearson.

Deckard, R. (2023) ‘What Are Ethics in AI’, AI and Ethics Journal, 5(2).

European Commission (2019) High-Level Expert Group on Artificial Intelligence: Ethics Guidelines for Trustworthy AI. Brussels: European Commission.

Finn, M. and Shilton, K. (2023) ‘Ethics governance development: The case of the Menlo Report’, Social Studies of Science, 53(3), pp. 315–340.

Fjeld, J. et al. (2020) Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI. Berkman Klein Center Research Publication.

IBM (no date) Principles for Trust and Transparency. Armonk, NY: IBM.

SAP (no date) Guiding Principles for Artificial Intelligence. Walldorf: SAP.




<img width="600" height="400" alt="image" src="https://github.com/user-attachments/assets/fc27c23c-18ce-43a8-87e0-488ca4dddb3c" />




[Back to the top](#research-methods-and-professional-practice)



[Go to main Menu](https://narchondas.github.io/)


