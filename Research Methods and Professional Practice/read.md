# Research Methods and Professional Practice

This is my learning experience from Module 6 of the MSc Artificial Intelligence

<img width="212" height="178" alt="image" src="https://github.com/user-attachments/assets/ba8d89bb-fd41-499e-ac3c-92f05c3557d5" />



Use the following links to navigate to the start of the sections


[1. Learning outcomes and action plan PDP](#learning-outcomes)

[2. Collaborative Discussions](#collaborative-discussions)

[3. Seminar and formative activities](#seminar-and-formative-activities)




## Learning outcomes


### In the module Research Methods and Professional Practice I shall:

*Acquire the ability to study and reflect on key principles and methods in research based on the scientific method and relevant to various disciplines.

*Acquire the ability to examine various research strategies and designs as applicable to projects at hand.

*Acquire the ability to develop research competencies, in particular those relating to the collection and analysis of data types to enable a critical design and evaluation of independent research.

*Have the opportunity to take a reflective and independent approach to the learning process.


### On completion of the module Research Methods and Professional Practice, I will be able to:

*Appraise the professional, legal, social, cultural and ethical issues that affect computing professionals.

*Appraise the principles of academic investigation, applying them to a research topic in the applicable computing field.

*Evaluate critically existing literature, research design and methodology for the chosen topic, including data analysis processes.

*Produce and evaluate critically a research proposal for the chosen topic.


<img width="1000" height="700" alt="image" src="https://github.com/user-attachments/assets/bf246f8d-495f-4a6d-beba-33406b2242d7" />



[Back to the top](#research-methods-and-professional-practice)



## Professional Skills matrix and action plan PDP


### Self-assessment

* Very motivated to learn about Research Methods and Professional Practice and advance with my MSc.

* Focused on timely delivery of good quality assignments.

* Good with time management, note taking and studying new material.



### Set goals

* Study and reflect on key principles and methods in research based on the scientific method and relevant to various disciplines.

* Examine various research strategies and designs as applicable to projects at hand.

* Develop research competencies, in particular those relating to the collection and analysis of data types to enable a critical design and evaluation of independent research.

* Regularly update my GitHub.


### Develop strategies

* Study the required and additional reading of each Module Unit.

* Take e-notes and highlight the most important sections.

* Understand the formative activities and test my knowledge with examples.

* Plan in advance for the assignments.

* Make sure there is sufficient time to fine-tune assignments.

* Gather feedback from tutor and peers.


### Gather resources

* Study all the required and additional resources of each Module Unit. 

* Prepare seminar activities.

* Perform all formative activities.

* Seek other explanatory reliable material on internet (websites, videos, classes etc).

* Reach out to the tutor.


### Create timeline

* Study all required and additional material every week.
  
* Plan in advance the assignments.

* Timely deliver the modelling assignment. 

* Timely submit my e-portfolio.


### Track progress and revise

* Regularly monitor if targets are met.

* Reconsider strategy, if needed.

* Monitor the timely delivery of good quality assignments.



<img width="716" height="428" alt="image" src="https://github.com/user-attachments/assets/7378e68e-2a58-486d-8cda-47a42c8e0627" />



[Back to the top](#research-methods-and-professional-practice)



## Collaborative discussions

### Initial, summary posts & response to peers' posts


### Collaborative Discussion 1


**Codes of Ethics and Professional Conduct**


**Initial Post**

The Blocker Plus case study, provided by the Association of Computing Machinery (ACM), illustrates the ethical complexity of deploying machine learning systems in socially sensitive contexts. While the system initially aligned with CIPA’s (Children’s Internet Protection Act) legal obligations, the developers breached key provisions of the BCS (British Computer Society) Code of Conduct. Activist groups exploited Blocker Plus’s feedback mechanism to manipulate the machine learning model, leading it to classify legitimate content on gay and lesbian marriage, vaccination, climate change, and other socially relevant topics as harmful to children. The failure to prevent bias and safeguard against malicious data input constituted a violation of the principles of Public Interest, particularly the duty to avoid discrimination and to promote inclusivity (BCS, 2021).

Furthermore, the insufficient transparency and lack of public disclosure regarding the model’s limitations contravened the requirement for Professional Competence and Integrity, as responsible practitioners must ensure compliance with legislation and uphold honesty in technical reporting. The decision to preserve a corrupted model, rather than mitigate harms, also breached their Duty to Relevant Authority by neglecting due diligence and public accountability.

Ethically, this case study underscores the centrality of the principles of fairness, accountability, and transparency to contemporary AI governance frameworks (Fjeld et al., 2020; Finn and Shilton, 2023). It also resonates with the European Commission’s High-Level Expert Group on AI, which stresses the necessity of human oversight and explainability as safeguards against algorithmic discrimination. As Kluge Corrêa et al. (2023) observe, the transition from voluntary ethics to binding AI regulation reflects a growing recognition that professional responsibility must be matched by enforceable accountability. This case study exemplifies that professionalism in computing extends beyond legal compliance to the moral stewardship of technology’s societal impact.

 
#### References


BCS (2021) Code of Conduct.

Fjeld, J. et al. (2020) Principled Artificial Intelligence. Berkman Klein Center.

Finn, M. and Shilton, K. (2023) ‘Ethics Governance Development: The Menlo Report’, Social Studies of Science, 53(3), pp. 315–340.



**Summary Post**



The Blocker Plus case study, published by the Association of Computing Machinery (ACM), exemplifies the ethical fragility of machine learning systems deployed in socially sensitive contexts. Although the system initially complied with the Children’s Internet Protection Act (CIPA), its developers violated core principles of the BCS (2021) Code of Conduct by failing to prevent bias, safeguard data integrity, and ensure accountability. Activist groups exploited the feedback mechanism to corrupt the model through malicious data injection, leading it to classify legitimate content, such as LGBTQ+ rights, vaccination, and climate change, as harmful. This incident underscores how insufficient resilience against adversarial manipulation and weak validation mechanisms can translate directly into discriminatory outcomes (Gomez et al., 2024).

Beyond technical failure, the case reveals a deeper ethical deficit in AI system design. As Rahwan (2018) argues, ethical resilience demands that systems anticipate manipulation and adapt to evolving social contexts rather than rely on static principles. The divergence between ethics by design and ethics in practice persists, often aggravated by institutional pressures prioritising speed and functionality over responsibility (Morley et al., 2021).

Moreover, the incident highlights the need for participatory governance frameworks that align professional ethics with democratic oversight (Whittaker et al., 2021). Embedding human oversight, transparency, and explainability within AI governance remains essential for sustaining public trust (Fjeld et al., 2020; Ribeiro et al., 2025). Yet, as Robles and Mallinson (2025) observe, fragmented ethical guidelines are inadequate without enforceable accountability structures and proactive organisational practices.

Ultimately, Blocker Plus illustrates that ethical AI governance must extend beyond legal compliance. It requires continuous risk assessment, adversarial testing, and transparent incident response protocols, ensuring that the moral stewardship of technology is embedded not only in codes of conduct but in the very architecture of AI systems.

 

#### References

BCS (2021) Code of Conduct. London: British Computer Society.

Fjeld, J., Achten, N., Hilligoss, H., Nagy, A. and Srikumar, M. (2020) Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI. Cambridge, MA: Berkman Klein Center for Internet & Society.

Gomez, J.F., Shukla, A., Gómez, S. and Ruiz, C. (2024) ‘Algorithmic arbitrariness in content moderation’, Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT '24), Rio de Janeiro, Brazil, 3–6 June. New York: Association for Computing Machinery, pp. 2234–2253. https://doi.org/10.1145/3630106.3659036

Morley J., Elhalal A., Garcia F, Kinsey L., Mökander J., Floridi L. (2021) Ethics as a Service: A Pragmatic Operationalisation of AI Ethics. Minds Mach (Dordr).  https://doi.org/10.1007/s11023-021-09563-w

Rahwan, I. (2018) Society-in-the-loop: programming the algorithmic social contract. Ethics Inf Technol 20, 5–14 (2018). https://doi.org/10.1007/s10676-017-9430-8

Ribeiro, D., Rocha, T., Pinto, G., Cartaxo, B., Amaral, M., Davila, N. and Camargo, A. (2025) ‘Toward effective AI governance: a review of principles’, arXiv preprint, arXiv:2505.23417. https://doi.org/10.48550/arXiv.2505.23417

Robles, P. and Mallinson, D.J. (2025) ‘Advancing AI governance with a unified theoretical framework: a systematic review’, Perspectives on Public Management and Governance, gvaf013, pp. 1–15. https://doi.org/10.1093/ppmgov/gvaf013

Whittaker, M., Crawford, K., Dobbe, R., Fried, G., Kaziunas, E., Mathur, V. and Schwartz, O. (2021) The AI Now Report 2021: Participatory Governance in AI Systems. New York: AI Now Institute.



**Response to my peers' posts**



I fully agree with your analysis that the Accessibility in Software Development case highlights how disregarding ethical principles can lead to exclusion and reputational harm. As you rightly note, failure to comply with the ACM and BCS codes, especially their requirements to promote fairness, non-discrimination, and equal access, undermines the very social purpose of computing. The ethical obligation of inclusiveness extends beyond technical accessibility as it also concerns the broader duty to ensure that digital systems do not marginalise individuals or communities through design choices, biased data, or neglect of diverse needs.

In this respect, inclusiveness represents both a moral imperative and a professional standard. The BCS Code of Conduct (2021) explicitly requires computing professionals to promote equal access to the benefits of IT and to act without discrimination on any grounds. This obligation aligns with the principles of fairness and human well-being articulated in the ACM Code of Ethics (2025) and with the European Commission’s High-Level Expert Group on AI, which emphasises diversity, accountability, and human oversight as safeguards against exclusion.

As I discussed in relation to the Blocker Plus case, ethical lapses can also emerge in algorithmic contexts where models are manipulated to suppress content on sensitive issues such as same-sex marriage or climate change. Such incidents reveal that inclusiveness must be embedded not only in user interfaces but also in data governance, model transparency, and participatory design, ensuring that technology serves all sectors of society equitably.



#### References

ACM (2025) Code of Ethics and Professional Conduct.

BCS (2021) Code of Conduct.

European Commission (2020) Ethics Guidelines for Trustworthy AI.



I fully agree with your analysis of the Medical Implant Risk Analysis case, which illustrates how the company’s failure to anticipate technical and ethical risks represents not merely a design flaw but a breach of professional responsibility. As you rightly observe, the hard-coded values constitute a neglect of due diligence, directly contravening the BCS Code of Conduct (BCS, 2022) requirement to show due regard for public health, safety, and well-being. However, the company’s ethical duties extend beyond technical competence: they encompass a broader corporate responsibility toward employees, customers, and society at large.

From an internal perspective, management has a duty to foster an ethical culture that empowers engineers to report design flaws without fear of reprisal, thereby reinforcing professional integrity and competence (BCS, 2022). Externally, the company must ensure that its customers, especially vulnerable patients, are treated not merely as consumers but as stakeholders whose safety, autonomy, and dignity are paramount. This aligns with Nickel’s (2011) argument that direct computer-patient interfaces demand heightened ethical attention to maintain trust and uphold patient autonomy. Transparent communication about product risks and the inclusion of diverse user perspectives in testing and evaluation are therefore ethical imperatives.

More broadly, inclusiveness must guide the design and deployment of medical technologies. As emphasised by Pasricha (2022), ethical digital medicine design requires embedding accountability and inclusivity throughout the lifecycle of medical IoT systems. By integrating these principles, transparency, inclusiveness, and accountability into their corporate ethos, companies can ensure that technological innovation serves not only commercial objectives but also the collective welfare and trust of society (BCS, 2022).


 
#### References

BCS, The Chartered Institute for IT (2022) BCS Code of Conduct for Members.

Nickel, P.J. (2011) ‘Ethics in e-trust and e-trustworthiness: the case of direct computer-patient interfaces’, Ethics and Information Technology, 13(4), pp. 355–363.

Pasricha, S. (2022) ‘Ethics for Digital Medicine: A Path for Ethical Emerging Medical IoT Design.’  Arxiv. https://doi.org/10.48550/arXiv.2210.12007



Your discussion of the Abusive Workplace Behaviour case powerfully illustrates how breaches of ethical and professional standards extend beyond individual misconduct to systemic organisational failure. I fully concur that both Max’s actions and Jean’s inaction violate fundamental principles of the ACM and BCS Codes of Conduct, undermining professional integrity and the ethical fabric of the workplace (Gotterbarn, Miller and Rogerson, 2018; BCS, 2021). In such cases, ethical responsibility cannot be viewed as hierarchical but collective, shared across all levels of management and technical staff.

A crucial dimension of this case concerns inclusiveness and respect for diversity. The BCS Code (2021, clauses 1b and 2d) explicitly mandates members to value alternative viewpoints and to avoid unfair discrimination. By tolerating gender-based aggression and denying authorship credit, the organisation not only breaches these duties but also erodes the inclusive culture essential for innovation, trust, and well-being (Johnson, 2020). Ethical computing practice, therefore, requires cultivating environments where all professionals, regardless of gender, ethnicity, or orientation, can contribute freely and safely.

Moreover, psychological safety constitutes an ethical prerequisite for professional competence. Employees who fear reprisal or humiliation cannot exercise critical judgment, creativity, or accountability, which are values central to both ACM principle 1.1 and the BCS commitment to public welfare (ACM, n.d.; Gotterbarn, Miller and Rogerson, 2018). The failure to intervene against abuse thus represents not merely managerial negligence but a profound ethical lapse. Promoting inclusiveness and dignity at work must be recognised as an ongoing ethical duty and a cornerstone of professional excellence in computing.


 
#### References

ACM (n.d.) Case Study: Abusive Workplace Behaviour. Available at: https://www.acm.org/code-of-ethics/case-studies/abusive-workplace-behavior (Accessed: 25 October 2025).

BCS (2021) Code of Conduct. British Computer Society. Available at: https://www.bcs.org/membership/become-a-member/bcs-code-of-conduct/ (Accessed: 25 October 2025).

Gotterbarn, D., Miller, K. and Rogerson, S. (2018) ‘ACM Code of Ethics and Professional Conduct’, Communications of the ACM, 61(1), pp. 21–28.

Johnson, D.G. (2020) Computer Ethics. 5th edn. Harlow: Pearson.



<img width="737" height="445" alt="image" src="https://github.com/user-attachments/assets/d4eb2ce3-0933-4bf4-bdbe-a18cf7b89de0" />



[Back to the top](#research-methods-and-professional-practice)



## Seminar and formative activities 



### Unit Formative Activities


**Unit 1**


**Towards a balanced framework for governance and professional responsibility**

The unprecedented expansion of AI technologies has ushered in what Corrêa et al. (2023) term the “AI ethics boom”, a period marked by a proliferation of guidelines, recommendations, and normative frameworks intended to direct the responsible use of AI. While this global momentum reflects growing awareness of the societal implications of automation and algorithmic decision-making, it simultaneously exposes a pressing challenge: how to maintain ethical integrity and public trust while enabling innovation and competitiveness. The ensuing discussion explores the evolution of AI ethics, the different forms of regulatory and self-regulatory governance, and proposes a variable-geometry model capable of harmonising binding obligations with voluntary principles.

AI technologies are now deeply embedded in social, economic, and political systems, influencing healthcare, education, finance, and governance. Corrêa et al. (2023) identify over 200 ethical frameworks worldwide, revealing a remarkable convergence around eight recurrent principles: fairness and non-discrimination, privacy, accountability, transparency, safety and security, professional responsibility, human control of technology, and promotion of human values. These principles echo the findings of Fjeld et al. (2020), who mapped consensus across diverse institutional settings, confirming fairness, transparency, and accountability as near-universal ethical anchors.

The acceleration of AI development, particularly in generative models and predictive analytics, necessitates that these principles move beyond theoretical ethical ideals and acquire enforceable status. As Dawson (2015) observed regarding research ethics, integrity, honesty, and respect for participants and intellectual property are not optional virtues but professional duties. In AI practice, this translates into ensuring that algorithms do not perpetuate bias, violate privacy, or undermine human autonomy. Deckard (2023) further emphasises that AI ethicists must possess interdisciplinary literacy, that is, combining technical proficiency with philosophical, legal, and sociological insight, to mediate between innovation and societal good.

Historically, the governance of AI began as voluntary self-regulation. Technology firms, research institutions, and professional bodies developed codes of conduct and internal review procedures in the absence of formal legislation. The BCS (2021) Code of Conduct, for instance, underscores the duty of computing professionals to act with integrity, safeguard public interest, and promote inclusion and fairness. Such self-commitments aligned with what Corrêa et al. (2023) classify as soft law or non-binding guidelines, designed to integrate ethical principles into practice without imposing coercive sanctions.

However, voluntary codes alone have proven insufficient to mitigate systemic risks such as algorithmic discrimination, misinformation, and opacity. The Menlo Report examined by Finn and Shilton (2023) offers a paradigm shift: recognising that data produced by or about machines can have direct human impact. This ontological realignment, treating network data as ethically sensitive, bridged the gap between technological research and human subjects protection. Consequently, ethics governance shifted from voluntary moral guidance to a more formalised, institutional process embedded within law and public policy.

Across jurisdictions, different trajectories have emerged in the institutionalisation of AI ethics. The EU has assumed a leadership role through the AI Act, a risk-based regulatory framework designed to ensure that AI systems placed on the market are transparent, explainable, and safe. The Act draws heavily upon the High-Level Expert Group on Artificial Intelligence (European Commission, 2019) and embodies a legally binding model, a hard-law approach, that mandates compliance with ethical principles. Its flexibility lies in its proportionality mechanism: high-risk applications (e.g., biometric surveillance, recruitment) face stringent requirements, while low-risk innovations remain less constrained. Nevertheless, critics argue that excessive regulatory burdens could stifle innovation and deter investment, echoing concerns expressed in Fjeld et al. (2020) about balancing accountability with technological progress.

In contrast, the US seems to have prioritised innovation and market dynamism. Federal policy relies primarily on soft law, voluntary standards and guidance such as the National Institute of Standards and Technology (NIST) AI Risk Management Framework, emphasising transparency and industry self-governance rather than prescriptive regulation. China, meanwhile, presents a more complex case: a hybrid model combining state-centric control with strategic promotion of AI leadership. Ethical oversight is closely intertwined with social governance objectives, raising questions about privacy, autonomy, and freedom (Corrêa et al., 2023).

Beyond national frameworks, international institutions such as UNESCO, the OECD, and the G7 have advanced convergent principles promoting human rights, fairness, and accountability. These efforts signal the gradual emergence of a global ethical infrastructure, yet one still fragmented by divergent political and economic priorities.
Given the diversity of legal traditions and innovation ecosystems, a uniform, binding, and comprehensive global framework remains impractical. A pragmatic solution lies in adopting a variable-geometry model, a multi-layered structure combining binding regulations, non-binding recommendations, and voluntary professional codes. Under such model, legally enforceable instruments would target high-risk applications with significant potential for harm (e.g., medical diagnostics, autonomous vehicles), ensuring transparency, auditability, and accountability. Complementing these, soft law instruments such as OECD recommendations or corporate principles like IBM’s Trust and Transparency Guidelines (n.d.) and SAP’s (n.d.) AI Principles, would encourage ethical alignment in lower-risk contexts while allowing agility in technological experimentation.

The strength of such a framework lies in its adaptability. Ethical principles are inherently dynamic.  As AI evolves, so too must the mechanisms that govern it. Continuous review, stakeholder participation, and cross-sector collaboration are therefore vital to maintain legitimacy and public confidence. As Deckard (2023) notes, effective AI ethicists must also participate in public policy debates, bridging the gap between technical design and democratic accountability.

The EU possesses a unique capacity to shape global standards through market power, through the so-called “Brussels effect.” By making compliance with its ethical and legal standards a prerequisite for access to the Single Market, the EU can indirectly compel external actors, including third-country providers, to adhere to high levels of transparency, safety, and fairness. This extraterritorial influence, already observed in data protection through the General Data Protection Regulation (GDPR), could similarly extend to AI, promoting a race to the top in ethical compliance.

Nevertheless, the EU’s approach must remain sufficiently flexible to accommodate innovation while maintaining vigilance against potential harms. Excessive rigidity could drive technological development outside its jurisdiction, undermining both competitiveness and ethical influence. A balance between innovation-friendly regulation and robust enforcement is therefore indispensable.

For computing professionals, the convergence of ethical and legal expectations demands a renewed commitment to professional responsibility. The BCS (2021) and the Menlo framework emphasise the dual obligation to comply with legal norms and to act in the public interest, even when the law remains silent. This entails developing AI systems that are explainable, repeatable, and accountable, qualities that reinforce public trust and reduce systemic risks. Ethical literacy must thus become a core competence of the digital profession, alongside technical proficiency.

In conclusion, the governance of AI should evolve towards a variable-geometry structure in which binding legal norms secure essential safeguards against harm, soft-law instruments promote shared ethical alignment, and professional codes cultivate individual accountability. Such a layered system would ensure that innovation proceeds in a manner consistent with human rights, democratic values, and social welfare. The EU AI Act can serve as a cornerstone for this model, provided it remains adaptive and internationally engaged. Ultimately, the ethical trajectory of AI will depend not solely on laws or algorithms, but on the collective integrity and professional responsibility of those who design, deploy, and oversee intelligent systems.

**References**

BCS (2021) The Chartered Institute for IT: Code of Conduct. London: BCS. 

Corrêa, N.K. et al. (2023) ‘Worldwide AI ethics: A review of 200 guidelines and recommendations for AI governance’, Patterns, 4(10).

Dawson, C. (2015) Projects in Computing and Information Systems: A Student’s Guide. Harlow: Pearson.

Deckard, R. (2023) ‘What Are Ethics in AI’, AI and Ethics Journal, 5(2).

European Commission (2019) High-Level Expert Group on Artificial Intelligence: Ethics Guidelines for Trustworthy AI. Brussels: European Commission.

Finn, M. and Shilton, K. (2023) ‘Ethics governance development: The case of the Menlo Report’, Social Studies of Science, 53(3), pp. 315–340.

Fjeld, J. et al. (2020) Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI. Berkman Klein Center Research Publication.

IBM (no date) Principles for Trust and Transparency. Armonk, NY: IBM.

SAP (no date) Guiding Principles for Artificial Intelligence. Walldorf: SAP.




<img width="600" height="400" alt="image" src="https://github.com/user-attachments/assets/fc27c23c-18ce-43a8-87e0-488ca4dddb3c" />




[Back to the top](#research-methods-and-professional-practice)



[Go to main Menu](https://narchondas.github.io/)


