# Knowledge Representation and Reasoning

This is my learning experience from Module 4 of the MSc Artificial Intelligence

![image](https://github.com/user-attachments/assets/53e67fe2-1df6-4ee6-b3a2-5ca390aa23cb)


Use the following links to navigate to the start of the sections

[1. Professional Skills matrix and action plan PDP](#professional-skills-matrix-and-action-plan-PDP)

[2. Collaborative Discussions](#collaborative-discussions)

[3. Seminar and formative activities](#seminar-and-formative-activities)

[4. Case Study Review](#case-study-review)

[5. Modelling Assignment](#modelling-assignment)

[6. Individual Reflection](#individual-reflection)



## Learning outcomes


### In the module Knowledge Representation and Reasoning I shall:

*Discuss how to define knowledge and explain the concept of Knowledge Representation and Reasoning and its relationship to data and information.
  
*Gain an appreciation of the importance and rationale for employing formal approaches to Knowledge Representation and Reasoning.

*Demonstrate an understanding of the basics of logic and its application to Knowledge Representation and Reasoning problems.

*Acquire an understanding of the professional competence required to be able to identify and examine Knowledge Representation and Reasoning technologies for Artificial Intelligence applications.


### On completion of the module Knowledge Representation and Reasoning, I will be able to:

*Critique the need for formal approaches to Knowledge Representation and Reasoning.
  
*Review critically properties of a knowledge-based system.

*Appraise critically modelling techniques for knowledge representation and reasoning.

*Examine and incorporate different modelling approaches to solving Knowledge Representation and Reasoning problems.


![image](https://github.com/user-attachments/assets/96e7c730-c504-4a3e-bb0f-304ea7326c24)


[Back to the top](#knowledge-representation-and-reasoning)



## Professional Skills matrix and action plan PDP


### Self-assessment

* Very motivated to learn about Knowledge Representation and Reasoning and advance with my MSc.

* Focused on timely delivery of good quality assignments.

* Good with time management, note taking and studying new material.

* Lack of a STEM background, so more difficult to get quickly familiar with the concepts and perform the formative acitivities i.e. coding and Protege exercises.


### Set goals

* Learn about the importance and rationale for employing formal approaches to Knowledge Representation and Reasoning.

* Acquire an understanding of the professional competence required to be able to identify and examine Knowledge Representation and Reasoning technologies for Artificial Intelligence applications.

* Understand the basics of logic and its application to Knowledge Representation and Reasoning problems.

* Be able to appraise critically modelling techniques for Knowledge Representation and Reasoning.

* Regularly update my GitHub.


### Develop strategies

* Study the required and additional reading of each Module Unit.

* Take e-notes and highlight the most important sections.

* Understand the formative activities and test my knowledge with examples.

* Prepare all seminar activities.

* Perform the required formative activities of the Module Unit.

* Plan in advance for the assignments.

* Make sure there is sufficient time to fine-tune assignments.

* Gather feedback from tutor and peers.


### Gather resources

* Study all the required and additional resources of each Module Unit. 

* Prepare seminar activities.

* Perform all formative activities (i.e. Protege exercises).

* Seek other explanatory reliable material on internet (websites, videos, classes etc).

* Reach out to the tutor.


### Create timeline

* Study all required and additional material every week.
  
* Plan in advance the assignments.

* Timely deliver the modelling assignment. 

* Timely submit my e-portfolio.


### Track progress and revise

* Regularly monitor if targets are met.

* Reconsider strategy, if needed.

* Monitor the timely delivery of good quality assignments.


[Back to the top](#knowledge-representation-and-reasoning)



## Collaborative discussions

### Initial, summary posts & response to peers' posts


### Collaborative Discussion 1



**Knowledge Representation is a recent phenomenon – it only became a topic of discussion with the development of computing technology and the need to represent knowledge in computer systems. Discuss this assertion. Do you agree or disagree with this opinion? Justify your position, supported with at least two academics references.  How is reasoning related to knowledge representation (KR)? Is KR still useful without reasoning support? Justify your answer supported by two academic references.**

**Initial Post**

The assertion that knowledge representation (KR) is a recent phenomenon, emerging only with the development of computing technology, is not entirely accurate.  While the formal study of KR as a subfield of artificial intelligence (AI) began in the mid-20th century, the human need to represent, store, and transmit knowledge is ancient and predates computers by millennia.

Humans have always sought ways to externalise and perpetuate knowledge beyond individual memory.  One of the most striking examples is the use of Egyptian hieroglyphics, which date back to around 3300 BC.  Hieroglyphics served not only as a means of communication but also as a sophisticated system for encoding religious, historical, and administrative knowledge in a durable, interpretable form.  This demonstrates that the need to represent knowledge systematically is deeply rooted in human culture.  As Weststeijn (2011) put it, ‘[c]ommunication through images is more fundamental than that through alphabetic signs’.

Beyond pictorial and written systems, the development of formal logic by philosophers and mathematicians such as Aristotle, Leibniz, and later George Boole, further exemplifies pre-computer knowledge representation. These formal systems were designed to capture the structure of reasoning and argumentation, providing a symbolic language for expressing complex ideas and relationships (Brachman & Levesque, 2004).

With the advent of computers, the study of KR gained new urgency and direction. Computers made it possible to automate reasoning and manipulate large bodies of knowledge efficiently.  This led to the development of explicit KR formalisms such as semantic networks, frames, and ontologies, which are central to AI research today (Brachman & Levesque, 2004).

Reasoning is fundamentally linked to knowledge representation (KR).  KR structures and encodes information so that a system can "understand" it, while reasoning uses this structured knowledge to draw inferences, solve problems, and make decisions (Lee, 2024).  In essence, KR provides the what (facts, relationships, rules), and reasoning provides the how (deriving new information, making conclusions).

KR alone, without reasoning support, is limited in usefulness.  While it allows for the organisation, storage, and retrieval of information, the real power of KR is realised when reasoning mechanisms are applied.  Reasoning transforms static data into actionable insights by enabling systems to infer new knowledge, detect inconsistencies, and make decisions.  Without reasoning, KR is reduced to a sophisticated database, lacking the ability to generate new conclusions or adapt to new situations.


#### References

Brachman, R. J., & Levesque, H. J. (2004). Knowledge Representation and Reasoning. Morgan Kaufmann.

Davis, E., & Marcus, G. (2015). "Commonsense Reasoning and Commonsense Knowledge in Artificial Intelligence." Communications of the ACM, 58(9), 92–103.

Lee A. (2024), Knowledge Representation and Reasoning in AI: Analyzing Different Approaches to Knowledge Representation and Reasoning in Artificial Intelligence Systems, Journal of Artificial Intelligence Research, Volume 4 Issue 1

Weststeijn, T. (2011) 'From hieroglyphs to universal characters. Pictography in the early modern Netherlands', Netherlands Yearbook for History of Art / Nederlands Kunsthistorisch Jaarboek, 61(1), pp. 238–281.


**Summary Post**


The assertion that knowledge representation (KR) is a recent phenomenon, emerging only with the development of computing technology, is not entirely accurate.  As Weststeijn (2011) notes, the use of visual language in pictography, such as Egyptian hieroglyphics, demonstrates that KR has strong historic roots.  Hieroglyphics served not only as a means of communication but also as a sophisticated system for encoding religious, historical, and administrative knowledge in a durable, interpretable form.  The human need to represent, store, and transmit knowledge is ancient, predating computers by millennia.  Formal logic, developed by philosophers such as Aristotle and Leibniz, further exemplifies pre-computer KR, providing a symbolic language for expressing complex ideas and relationships (Brachman & Levesque, 2004).  

KR structures and encodes information, allowing reasoning processes to draw inferences and make decisions (Lee, 2024). Reasoning is critical for the functionality and utility of KR, and its contemporary understanding in computing is rooted in its ability to enable automated reasoning with systems like semantic networks and ontologies (Brachman and Levesque, 2004).  Moreover, KR's utility is not limited to formal reasoning support, as structured data systems serve critical roles in numerous fields, such as archiving and retrieval in libraries (Davis et al., 1992).  

KR is a continuation of ancient practices, with a distinct conceptual foundation, rather than a new invention. The use of formal logic, for instance, not only represented knowledge but also guided reasoning processes in mathematics and philosophy (Russell & Norvig, 2010). Ultimately, KR provides the structures necessary for reasoning processes, enabling systems to infer new information, detect inconsistencies and make decisions, and thus transcending its computational applications to root deeply in human efforts to encode and re-utilise knowledge. 



#### References

Brachman, R. J., & Levesque, H. J. (2004). Knowledge Representation and Reasoning. Morgan Kaufmann.

Davis, R., Shrobe, H., & Szolovits, P. (1992). What is a knowledge representation? AI Magazine, 14(1), 17-33

Davis, E., & Marcus, G. (2015). "Commonsense Reasoning and Commonsense Knowledge in Artificial Intelligence." Communications of the ACM, 58(9), 92–103.

Lee A. (2024), Knowledge Representation and Reasoning in AI: Analyzing Different Approaches to Knowledge Representation and Reasoning in Artificial Intelligence Systems, Journal of Artificial Intelligence Research, Volume 4 Issue 1

Russell, S., & Norvig, P. (2010). Artificial Intelligence: A Modern Approach (3rd ed.). Prentice Hall.

Weststeijn, T. (2011) 'From hieroglyphs to universal characters. Pictography in the early modern Netherlands', Netherlands Yearbook for History of Art / Nederlands Kunsthistorisch Jaarboek, 61(1), pp. 238–281.



#### Response to my peers' posts


I fully agree that knowledge representation (KR) is not a recent innovation but rather an ancient human practice rooted in the necessity to encode, preserve, and transmit knowledge.  Long before computational KR, early civilizations developed sophisticated symbolic systems—from Paleolithic cave paintings (~40,000 BCE) to Egyptian hieroglyphics (~3300 BCE)—that served as durable, interpretable repositories of cultural, religious, and administrative knowledge (Weststeijn, 2011). These systems underscore humanity’s enduring drive to externalise thought, a pursuit later refined through alphabets, formal logic (i.e. Aristotle’s syllogisms, Leibniz’s characteristica universalis), and mathematical notation (Brachman & Levesque, 2004).  Such historical precedents confirm that KR’s conceptual foundations predate computers by millennia and modern AI merely formalises these principles algorithmically.

However, while KR can exist as a standalone framework for organising and retrieving data (structured databases or ontologies) its full potential is realised only when paired with reasoning.  As Brachman and Levesque (2004) argue, KR structures (i.e. semantic networks, frames) provide the what (facts, rules, relationships), while reasoning enables the how (inference, problem-solving).  For instance, a medical ontology might encode disease-symptom relationships, but without reasoning (i.e. deductive logic or probabilistic inference), it cannot diagnose new cases or resolve contradictions (Davis et al., 1993). This aligns with Sowa’s (2000) observation that reasoning transforms static KR into dynamic intelligence, generating actionable insights (Sowa, 2000).

Reasoning may enhance KR by enabling inference and problem-solving, but KR alone still has value. Systems like databases and knowledge graphs show that structured KR works well for storing and finding information, even without advanced reasoning (Sowa, 2000). The real challenge for AI today is combining these modern tools with principles of clear representation to create systems that are both powerful and understandable.


#### References

Brachman, R. J., & Levesque, H. J. (2004). Knowledge Representation and Reasoning. Morgan Kaufmann.

Davis, R., Shrobe, H., & Szolovits, P. (1993). What is a Knowledge Representation? AI Magazine, *14*(1), 17–33.

Sowa, J. F. (2000). Knowledge Representation: Logical, Philosophical, and Computational Foundations. Brooks/Cole.

Weststeijn, T. (2011). The Visible World: Dutch Visual Culture and the Rise of Early Modern Science. Amsterdam University Press.



Your post provides a comprehensive overview of knowledge representation (KR) and its historical evolution.  I particularly appreciate how you have highlighted both the ancient roots of KR and its modern computational applications.  Examples of Sumerian cuneiform, Egyptian hieroglyphics, and Greek logical systems (Weststeijn, 2011) effectively demonstrate that the human impulse to systematically represent knowledge predates computing by millennia.  This historical continuity is crucial for understanding KR's fundamental role in human cognition and information processing.

Modern computing has amplified KR's importance through formalisation and scalability (Brachman & Levesque, 2004).  The transition from ancient pictograms to contemporary semantic networks and ontologies represents an evolution in form rather than a completely new concept.  This perspective aligns with Davis et al.'s (1992) view that KR serves as a bridge between human-understandable knowledge and computer-processable data.

However, a clear distinction has to be made between KR and reasoning, as KR's standalone utility for information organisation is significant.  Many practical applications, from database systems to digital archives, demonstrate that structured knowledge representation maintains value even without automated reasoning components.  This echoes Nilsson's (1998) argument that KR serves multiple purposes beyond just enabling machine inference.

One area that might warrant further exploration is how modern machine learning approaches interact with traditional KR methods.  The emergence of neural networks presents new opportunities and challenges for knowledge representation that build on these historical foundations.


#### References

Brachman, R. J., & Levesque, H. J. (2004). Knowledge Representation and Reasoning. Morgan Kaufmann.

Davis, R., Shrobe, H., & Szolovits, P. (1993). What is a Knowledge Representation? AI Magazine, 14(1), 17-33.

Nilsson, N. J. (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann.

Russell, S., & Norvig, P. (2010). Artificial Intelligence: A Modern Approach (3rd ed.). Prentice Hall.

Weststeijn, T. (2011). The Visible World: Dutch Visual Culture and the Rise of Early Modern Science. Amsterdam University Press.



![image](https://github.com/user-attachments/assets/668798c3-8875-48b4-8bfb-f729f82ed8ec)



### Collaborative Discussion 2



**How is an ontology defined in this thesis? Based on the identified definition, which language do you believe is the most useful to express ontologies that can be utilised by software agents on the WWW: KIF, OWL2, RDF or OWL-lite?**

**Initial Post**

An ontology is a structured, formal framework for representing knowledge about a domain, enabling logic-based reasoning to infer new information. It is defined, following Gruber (1993), as “a formal, explicit specification of a shared conceptualisation,” where conceptualisation denotes an abstract model of a phenomenon in the world (as cited in Nassim, 2022). Ontologies facilitate the sharing and reuse of knowledge by providing a semantic structure for communication between systems, humans, or both.

Ontologies are composed of classes (concepts/categories), properties (attributes/relationships), and individuals (instances of classes). These components form the basis for creating domain models that can be interpreted by machines and understood by humans. As Guarino, Oberle, and Staab (2009) argue, ontologies are key enablers of semantic interoperability, offering a common vocabulary and grammar for representing data.
The Semantic Web is a research domain aimed at transforming the current human-centric web into a machine-interpretable structure, thereby facilitating greater interoperability among web-based data sources. Unlike the conventional web, which is primarily designed for human comprehension and thus hampers seamless data exchange across systems, the Semantic Web seeks to enable automated information processing and integration (Bernstein et al., 2016, as cited in Nassim, 2022). 

Choosing an appropriate ontology language for the Semantic Web requires support for formal specification, logical inference, and interoperability. Among the options—KIF (Knowledge Interchange Format), OWL2, RDF (Resource Description Framework), and OWL-lite—OWL2 stands out as the most useful for this purpose.  OWL2 (Web Ontology Language 2), a W3C standard, offers a high level of logical expressiveness grounded in Description Logics, enabling decidable reasoning (Nassim, 2022). It integrates with RDF for data representation, while extending its capabilities for richer semantic modelling.

OWL Lite offers limited expressiveness, and KIF, though logic-rich, lacks native web support and adoption. OWL2, by contrast, underpins key semantic web applications such as DBpedia and semantic web services, where it facilitates automated reasoning and interoperability (Nassim, 2022 and UoEO, n.d.).

Therefore, OWL2 stands out as the most effective ontology language for enabling intelligent, scalable, and interoperable knowledge systems on the web.


#### References

Guarino, N., Oberle, D. and Staab, S. (2009) ‘What is an ontology?’, in Staab, S. and Studer, R. (eds.) Handbook on Ontologies. Berlin: Springer, pp. 1–171.

Nasim, T. M. (2022) 'Improving Ontology Alignment Using Machine Learning Techniques', ProQuest Dissertations & Theses.

UoEO (n.d.), Lecturecast, Module 4 Knowledge Representation and Reasoning, , Unit 9 Formalism Techniques and Applications.



**Summary Post** 

Ontologies serve as structured, formal frameworks for representing domain knowledge in a way that enables logic-based reasoning and semantic interoperability. Drawing from Gruber (1993), an ontology is defined as “a formal, explicit specification of a shared conceptualisation,” where conceptualisation refers to an abstract model of phenomena in the world (as cited in Nassim, 2022). This shared abstraction allows for coherent knowledge exchange between both machines and humans. As Guarino, Oberle, and Staab (2009) argue, ontologies provide the semantic 'scaffolding' necessary for interoperability across diverse systems by offering a common vocabulary and logic for data structuring.

Ontologies are fundamentally composed of three core components: classes (or concepts), properties (attributes or relationships), and individuals (instances of classes). This triadic structure facilitates the development of domain models that are both human-understandable and machine-interpretable. As discussed, such organisation is instrumental in achieving the objectives of the Semantic Web, a paradigm shift from a human-centric web to a machine-readable framework designed to enhance data integration and automated inference (Bernstein et al., 2016, as cited in Nassim, 2022).

Central to enabling the Semantic Web is the choice of an ontology language. Among the candidates—KIF, RDF, OWL Lite, and OWL2—OWL2 (Web Ontology Language 2) emerges as the most robust and widely adopted solution. A W3C standard grounded in Description Logics, OWL2 supports decidable reasoning while offering integration with RDF, thereby enhancing both expressiveness and interoperability (Nassim, 2022; UoEO, n.d.). Its structured profiles—EL (Expressive Language), QL (Query Language), and RL (Rule Language)—allow developers to tailor ontologies to specific reasoning and performance needs (Horrocks et al., 2003).

Given its expressive power, logical soundness, and modularity, OWL2 is well positioned to serve as the foundational language for intelligent, scalable, and interoperable Semantic Web applications, including DBpedia and complex data integration systems. Its broad adoption and alignment with W3C standards further reinforce its relevance and effectiveness for ontology-based knowledge representation in contemporary digital ecosystems.


#### References

Guarino, N., Oberle, D. and Staab, S. (2009) ‘What is an ontology?’, in Staab, S. and Studer, R. (eds.) Handbook on Ontologies. Berlin: Springer, pp. 1–17.

Gruber, T. R. (1993) ‘A translation approach to portable ontology specifications’, Knowledge Acquisition, 5(2), pp. 199–220.

Horrocks, I., Patel-Schneider, P. F. and van Harmelen, F. (2003) ‘From SHIQ and RDF to OWL: The making of a web ontology language’, Journal of Web Semantics, 1(1), pp. 7–26.

Nasim, T. M. (2022) Improving Ontology Alignment Using Machine Learning Techniques. Master’s Thesis. Arizona State University.

UoEO (n.d.). Lecturecast, Module 4 Knowledge Representation and Reasoning, Unit 9 Formalism Techniques and Applications.



#### Response to my peers' posts



I fully agree with the definition and stance presented in your post, highlighting ontologies as structured, formal frameworks designed to represent domain-specific knowledge explicitly, enabling logic-based reasoning and inference. As you rightly referenced, ontologies constitute a “formal, explicit specification of a shared conceptualisation” (Gruber, 1993, as cited in Nasim, 2022). This conceptualisation acts as an abstract, shared model of phenomena, laying the foundation for effective semantic communication and interoperability among diverse systems and human users alike (Guarino, Oberle, and Staab, 2009).

Ontologies systematically organise domain knowledge through the precise definition of classes, properties, and individuals. Classes represent general concepts, properties describe the relationships and attributes, and individuals instantiate specific occurrences of these concepts. This structural clarity significantly facilitates both knowledge sharing and automated reasoning, fundamental for realising the vision of the Semantic Web (Bernstein et al., 2016, cited in Nasim, 2022).

Your stance on OWL2 as the most suitable language for semantic web ontologies is equally justified. OWL2, a W3C-endorsed standard grounded in Description Logics, balances expressive power with computational practicality. Its compatibility with RDF enhances interoperability and enables powerful, decidable reasoning capabilities, essential for tasks like ontology alignment, data integration, and semantic web applications such as DBpedia (Nasim, 2022; UoEO, n.d.). Thus, OWL2 emerges clearly as the most robust, interoperable, and widely adopted language, ensuring effective, scalable semantic modelling across web-based platforms.

**References**

Guarino, N., Oberle, D. and Staab, S. (2009). Handbook on Ontologies. Springer, Berlin.

Nasim, T. M. (2022). Improving Ontology Alignment Using Machine Learning Techniques. ProQuest Dissertations & Theses.

UoEO (n.d.). Lecturecast, Module 4 Knowledge Representation and Reasoning, Unit 9 Formalism Techniques and Applications.



I fully concur with the definition of ontology and the rationale for adopting OWL2 as presented in your post. Drawing on the foundational perspective of Gruber (1993), ontologies are best understood as formal, explicit specifications of shared conceptualisations, where conceptualisation refers to an abstract model of a phenomenon in the world (as cited in Nasim, 2022). This definition encapsulates the essential role of ontologies as structured, machine-readable frameworks that enable logic-based reasoning and support the sharing and reuse of knowledge across heterogeneous systems and users.

Ontologies are composed of three core elements: classes (concepts or categories), properties (attributes and relationships), and individuals (instances of classes). These components form the semantic backbone of domain models that are both interpretable by machines and comprehensible to humans. As Guarino, Oberle, and Staab (2009) argue, ontologies are essential for achieving semantic interoperability, offering a common vocabulary and logic for structuring and integrating data.

The selection of a suitable ontology language for the Semantic Web must satisfy requirements for logical expressiveness, formal semantics, and seamless web integration. While RDF-Resource Description Framework provides a foundational syntax and KIF-Knowledge Interchange Format offers expressive formalism, only OWL2—grounded in Description Logics and endorsed by the W3C—successfully balances expressiveness with computational tractability. Its profiles (EL-Expressive Language, QL-Query Language, RL-Rule Language) allow trade-offs between reasoning complexity and scalability, thereby enabling real-world applications such as ontology alignment, data integration, and intelligent querying (Nasim, 2022; Horrocks et al., 2003).

Accordingly, OWL2 seems to be the most effective language for building intelligent, interoperable knowledge systems on the Web.

 
**References**

Gruber, T. R. (1993) ‘A translation approach to portable ontology specifications’, Knowledge Acquisition, 5(2), pp. 199–220.

Guarino, N., Oberle, D. and Staab, S. (2009) ‘What is an ontology?’, in Staab, S. and Studer, R. (eds.) Handbook on Ontologies. Berlin: Springer, pp. 1–17.

Horrocks, I., Patel-Schneider, P. F. and van Harmelen, F. (2003) ‘From SHIQ and RDF to OWL: The making of a web ontology language’, Journal of Web Semantics, 1(1), pp. 7–26.

Nasim, T. M. (2022) Improving Ontology Alignment Using Machine Learning Techniques. Master’s Thesis. Arizona State University.



![image](https://github.com/user-attachments/assets/f4e35001-436c-433a-ad84-bc820b68651e)



[Back to the top](#knowledge-representation-and-reasoning)


## Seminar and formative activities 



### Unit 1 Formative Activities

[Formative Activities](https://docs.google.com/document/d/1qy2-JkTt8nOwDD4dS7WUcCyUmRMH248uFYaOu8qBVzo/edit?tab=t.0)


### Unit 2 Seminar Activities

[Seminar Activities](https://docs.google.com/document/d/1kPT580-pRQKgM3Jem-5h6xzYFixuJs_QLBb0mE2gquw/edit?tab=t.0#heading=h.dsfkn9v4bvo7)


### Unit 3 Formative Activities

[Formative Activities](https://docs.google.com/document/d/1VMSFdsa_7XY52hMNlMpuKGZtq0U5c-FIMBUupN2KmQ4/edit?tab=t.0#heading=h.37it1c5glwum)


### Unit 4 Seminar Activities

[Seminar Activities 1](https://docs.google.com/document/d/16WviHtm9x-0CUEwWa9sOA_r_0JUWqomK/edit)

[Seminar Activities 2](https://docs.google.com/document/d/1l55DblBF4QvaZTvA1e2Sytv_V3jqxTVoyFfQjrsLDs0/edit?tab=t.0)


### Unit 5 Formative Activities

[Formative Activities](https://docs.google.com/document/d/1B5O7I42VkijiUux_ioErdBjmaw-mGZ7YGz4RYhs-tYY/edit?tab=t.0)


### Unit 6 Formative Activities

[Formative Activities](https://docs.google.com/document/d/147jzP4P6S2AxhWFLjuSvzJT96GOdvuK-HK5yBYoutnE/edit?tab=t.0)

**Exercise on Protege**

![image](https://github.com/user-attachments/assets/c3c35ba6-3eb3-4ddf-b0fc-6951637cba0c)


### Unit 8 Formative Activities

**Exercise on Protege**

![image](https://github.com/user-attachments/assets/ef92be40-9e11-41af-9347-bdf67ba51a5d)



![image](https://github.com/user-attachments/assets/51a7c4c0-6a1e-40c7-bc48-0e4a681b669a)



[Back to the top](#knowledge-representation-and-reasoning)




## Case Study Review 


**Analysis and Application of the Intelligence Task Ontology (ITO) in AI Benchmarking**


**This assignment requires students to critically analyse the paper ‘A curated ontology-based large-scale knowledge graph of artificial intelligence tasks and benchmarks’ by Kathrin Blagec (Blagec, K. et al. (2022), which introduces the Intelligence Task Ontology (ITO).**




The field of artificial intelligence (AI) is characterised by rapid expansion and increasing diversity in tasks, benchmarks, and evaluation metrics. This proliferation, while indicative of the field’s dynamism, has led to a fragmented research landscape that complicates systematic comparisons, meta-analyses, and the identification of research gaps.  As highlighted by Blagec et al. (2022), the absence of a unified, structured framework for organising and relating AI tasks has hindered efforts to track where novel methods succeed or fail, how progress is measured, and how advances interrelate.  The Intelligence Task Ontology and Knowledge Graph (ITO) was developed to address these challenges, providing a comprehensive, ontology-based, and manually curated resource that brings clarity and structure to the AI research ecosystem.  

As Debenham asserted “knowledge is the explicit functional associations between items of information and/or data” (Debenham, 1988), whereas ontologies are defined as formal, explicit specifications of shared conceptualisations, used to represent such reusable and sharable knowledge (Corcho et al., 2007).  

The main challenge ITO seeks to address is the fragmentation of AI knowledge. The proliferation of specialised subfields has resulted in an array of AI tasks, performance metrics and domain-specific benchmarks. This heterogeneity not only impedes cross-domain model comparisons but also leads to ambiguities in task definitions and metric interpretations.  While repositories like “Papers with Code” (PWC) have made significant contributions, their lack of a rigorous ontological structure limits their utility for systematic analysis. Moreover, the absence of standardised taxonomies and meta-research tools has rendered the process of tracking benchmark evolution and identifying research trends at least labour-intensive.

ITO’s primary aim is to enable ‘meta-research’ concerned with studying scientific research itself in terms of its methods, reporting, evaluation etc.  It can be utilised as a taxonomic resource for annotating and organising information in the AI-domain. Finally, the ITO knowledge network can serve as a practice-focused resource that allows developers to find, compare and select AI models to address complex use-cases for certain defined tasks, data types and application domains (Blagec et al., 2022).

The motivation for creating ITO is thus rooted in the need for a comprehensive, extensible, and interoperable framework that can support advanced meta-research and practical applications within the AI community. By offering a structured and curated knowledge graph (KG), ITO aims to facilitate precise analyses, enable integration with external data sources, and support ongoing expert curation to ensure the resource remains up-to-date.  By ontology curation is meant the process of manually reviewing and refining data to ensure its accuracy, completeness and consistency.

The field of AI has a long history of using open-source tools for creating AI systems and managing the large datasets needed to train neural networks.  ITO indirectly advances Open Science principles i.e. collaborative construction of KGs by volunteers or from open data, which enhances transparency and community engagement. Its use of standard formats like RDF/OWL (adhering to established W3C standards) supports interoperability and data reuse across fields. Open benchmarks, shared data, and clear methodologies foster reproducibility and innovation (Nickel et al., 2015). Paton et al. (2017) stress that open data, research, access, educational resources, and tools are essential for responsible and effective AI adoption in healthcare, while open access is crucial for disseminating knowledge to professionals and developers. Tolan et al. (2019) and Martínez-Plumed et al. (2019) note that open science framework is transparent and reproducible, with publicly available code and data, encouraging replication and community validation. 

The development of ITO was additionally guided by the following core objectives, among these, the ambition to provide a detailed and extensible taxonomy for AI tasks, benchmarks, and performance metrics, thereby enabling systematic meta-research and facilitating the integration of external data sources. The framework aspires to support collaborative curation and automated inference, ensuring that the ontology remains current and reflective of the latest developments in AI research.

Methodologically, ITO’s construction involved a combination of extensive manual curation and some automated data extraction.  The latter mainly originates from PWC database, which combines automated extraction of benchmarks and crowd-source annotation. The initial knowledge base was populated by inserting data from the PWC repository, which encompassed a vast array of benchmarks/publications.  While some automated extraction (originating from PWC) was essential for scalability, it was the subsequent extensive manual curation spanning several months that proved critical for resolving ambiguities, normalising performance metrics, and ensuring semantic clarity.  

AI tasks are modelled as “processes” and organised into a polyhierarchical structure with sixteen major parent classes, such as “Natural Language Processing” and “Vision Process.” This approach allows for the representation of cross-modal and overlapping tasks, which are increasingly prevalent in contemporary AI research. Importantly, ITO reuses and extends classes from established ontologies, thereby enhancing interoperability and facilitating future integration with external KGs. The quality and consistency of the ontology were validated using ontology evaluation tools, with metrics such as class richness, inheritance number, relationship richness and axiom/class ratio reported to ensure structural integrity. 

‘Validation and evaluation of a KG and/or ontology aims to assess whether the resource adequately and accurately covers the domain it intends to model, and whether it enables an efficient execution of the tasks it was designed for’ (Blagec et al. 2022). Commonly used criteria to evaluate ontologies include accuracy, clarity, completeness, conciseness, adaptability, computational efficiency and consistency.

The resulting resource is notable for its scope and depth. The current ITO version contains over 1,100 classes representing AI-processes and nearly 2,000 properties for performance metrics, with a total of 685,560 edges connecting various entities. This comprehensive mapping enables advanced queries through SPARQL, analyses, systematic tracking of benchmark evolution and metric usage, and improved discoverability of datasets, benchmarks, and models for both researchers/practitioners.

ITO’s process-centric modelling, in which tasks are defined as “processes,” offers a nuanced and flexible approach to classification. The polyhierarchical structure is particularly well-suited to the complex and overlapping nature of AI tasks, accommodating the reality that many contemporary AI challenges do not fit neatly into a single category. The integration of manual curation, while resource-intensive, ensures a high degree of semantic precision and data quality, addressing many of the shortcomings found in less rigorously curated repositories. Moreover, the use of RDF/OWL and alignment with established ontologies enhances interoperability, positioning ITO as a future-proof resource.

However, several limitations need to be equally considered. The reliance on manual curation, while beneficial for ensuring data quality, introduces a potential bottleneck as the volume and diversity of AI research continue to expand. It must be acknowledged that such resource-intensive processes may struggle to keep pace with the rapid evolution of the field.  Additionally, certain ontology design decisions—such as the creation of broad superclasses—may increase tangledness (degree of multihierarchical nodes -nodes with multiple super classes- in the class hierarchy) and complicate maintenance, deviating from some best practices in ontology engineering.  Therefore, potential delays due to manual curation might have a negative impact on ITO’s relevance for fast-evolving fields like generative AI, or tangled superclasses might complicate maintenance for specific tasks.

The initial dependence on PWC as the primary data source also introduces the risk of incomplete coverage, particularly for emerging tasks or benchmarks not yet included in major repositories. Finally, the complexity and depth of the ontology may present a barrier to entry for non-expert users, highlighting the need for user-friendly interfaces and comprehensive documentation.

Despite these challenges, the strengths of ITO are substantial. The rich semantic representation afforded by the KG structure enables the capture of intricate relationships among tasks, benchmarks, models, and metrics, supporting analyses that would be infeasible with simpler taxonomies. The extensibility and collaborative curation mechanisms further ensure that ITO can evolve in tandem with the field, maintaining its relevance and utility.

ITO’s potential applications are both diverse and impactful. By enabling systematic analyses of research progress and performance metric evolution, ITO supports evidence-based decision-making in AI development. Practitioners can leverage the ontology to efficiently identify relevant datasets, benchmarks, and state-of-the-art models, thereby enhancing reproducibility and informed model selection. The framework also provides a reference taxonomy for annotating datasets and benchmarks, promoting consistency and interoperability within the AI community. Furthermore, ITO’s structure lends itself to integration with AI development platforms and benchmarking dashboards, streamlining knowledge management and discovery processes.  ITO might change specific research practices, for example, by altering the way benchmarks are designed/reported or even reducing time for model selection by practitioners/researchers.

In addition, the implications for AI research practices are significant. By offering a mapping of tasks and metrics, ITO has the potential to contribute to the standardisation of benchmarking practices, reducing ambiguity and facilitating meaningful comparisons. The open and extensible nature of the ontology encourages community participation, fostering consensus on task definitions and evaluation standards. ITO could, for example, enhance explainability in healthcare for diagnostic model selection or streamline model selection for autonomous driving by mapping tasks like object detection linked to relevant benchmarks.  In particular, in the context of explainable AI (XAI), ontologies, like ITO, could help bridge the gap between complex AI models and human understanding by providing structured, interpretable explanations (Barredo Arrieta et al. 2019). Moreover, ITO’s extensibility supports integration with adjacent domains—such as biomedical research—enabling cross-domain knowledge transfer and interdisciplinary innovation.

However, it is important to recognise the risks and limitations. The adoption of ontology-based KGs for AI benchmarking is not without challenges. Issues such as the potential for misclassification due to evolving task definitions must be carefully managed. Additionally, the complexity of ontology maintenance necessitates ongoing investment in curation to ensure sustainability and accessibility.

In conclusion, ITO represents a significant advancement in the structuring and analysis of AI research. Its ontology-based KG approach provides a richly detailed, extensible, and interoperable framework for representing AI tasks, benchmarks, and performance metrics. While the reliance on manual curation presents scalability challenges, the strengths of ITO in enabling meta-research, standardisation, and collaborative development are considerable. As the ontology continues to evolve, it is anticipated that its impact on AI research practices, benchmarking standards, and interdisciplinary collaboration will become increasingly pronounced.


**References**

Barredo Arrieta, A. Díaz-Rodríguez, N. Del Ser, J. Bennetot, A. Tabik, S. Barbado, A. Garcia, S. Gil-Lopez, S. Molina, D. Benjamins, R. Chatila, R. Herrera, F. (2019) 
Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI, Information Fusion, Volume 58, Pages 82-115, https://doi.org/10.1016/j.inffus.2019.12.012. 

Blagec, K. et al. (2022) A curated, ontology-based, large-scale knowledge graph of artificial intelligence tasks and benchmarks. Available at https://doi-org.uniessexlib.idm.oclc.org/10.1038/s41597-022-01435-x 

Confalonieri, R. and Guizzardi G. (2024) On the multiple roles of ontologies in explanations for neuro-symbolic AI. Neurosymbolic Artificial Intelligence -1 (2024) 1–15, DOI 10.3233/NAI-240754

Corcho, O. Fernández-López, M. Gómez-Pérez, A. (2007) What Are Ontologies and How Can We Build Them? DOI: 10.4018/978-1-59904-045-5.ch003 

Debenham, J. K. (1988). Knowledge Systems Design. Prentice-Hall: Englewood Cliffs, NJ
Martínez-Plumed, F. Hernández-Orallo, J. (2019) Analysing Results from AI Benchmarks: Key Indicators and How to Obtain Them. Available at https://arxiv.org/pdf/1811.08186 [Accessed on 8 June 2025]

Nickel M., Murphy, K. Tresp, V. Gabrilovich, E. (2015) A Review of Relational Machine Learning for Knowledge Graphs. Centre for Brains, Minds & Machines. CBMM Memo No. 28 Available at https://cbmm.mit.edu/sites/default/files/publications/cbmm-memo-28.pdf [Accessed on 7 June, 2025]

Paton, C., Kobayashi, S. (2019) An Open Science Approach to Artificial Intelligence in Healthcare. Yearb Med Inform 2019:47-51 http://dx.doi.org/10.1055/s-0039-1677898 

Tolan, S. Pesole, A. Martínez-Plumed, F. Fernández-Macías E.  Hernández-Orallo, J. and Gomez, E. (2021) Measuring the Occupational Impact of AI: Tasks, Cognitive Abilities and AI Benchmarks (Extended Abstract). Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence (IJCAI-22) Journal Track. https://doi.org/10.1613/jair.1.12647



![knowledge-graph](https://github.com/user-attachments/assets/61a09a1e-424a-423b-80b1-dc047dbe9c64)



[Back to the top](#knowledge-representation-and-reasoning)




## Modelling Assignment

**In this assignment, you are tasked with employing the Protégé software (download or web version) to construct a prototype ontology as the backbone for an AI-driven job-matching service. This service matches job seekers with suitable employment opportunities, aligning with their individual skills, experiences, and preferences.**



In a dynamic, competitive and ever-changing labour market, the alignment between job seekers' profiles and available employment opportunities remains a persistent challenge. Traditional recruitment methods often fall short in capturing the multidimensional attributes, such as qualifications, hard/soft skills, preferences, and experiences, essential for effective job matching. To address this complexity, the proposed AI-driven job matching service leverages formal knowledge representation through ontology modelling, offering a semantic foundation for personalised and scalable job-matchmaking.  

Ontologies serve as a foundational component in knowledge-based AI systems. As defined by Gruber (1993), an ontology is "a formal, explicit specification of a shared conceptualisation," a definition widely cited in AI research and reaffirmed in Marquis et al. (2020) and Corcho et al. (2007). This structured approach enables consistent representation and inference of complex knowledge, allowing for machine-interpretable semantic relationships that transcend traditional data schemas.  

The developed ontology originates from owl:Thing and is structured around the core classes: Person (with subclasses Job Seeker, Employer), Job Offer, Education Level and Qualifications, IT Skills, Work Experience, Individual Skills, Personal Preferences, and Remuneration Level. Subclasses within Education Level and Qualifications include Secondary (e.g., General, Administrative Secretary), Technical (e.g., Maintenance, Logistics, Computing), and University (e.g., Bachelor’s Degree, Master’s Degree, PhD). IT Skills covers AI, Microsoft 365, databases, photo/video editing, and statistical packages. Work Experience is categorised as No Experience, Less than 5 Years, and More than 5 Years.  

Personal Preferences include desired Work Location (EU, Nationwide, Non-EU, Proximity), Sector (Public: EU Institutions, Local Authorities; Private: Agriculture, Healthcare, Retail, Construction-Building, Finance-Banking, Manufacturing), Type of Contract (Permanent, Temporary), and Recommendations from Previous Employers (Yes, No). Remuneration Level is segmented into four income brackets.  Individual Skills include sub-classes such as Negotiation, Policy, Managerial, Leadership-Initiative, Entrepreneurship, Administrative Work.

Object properties capture relationships between entities: isSeekingJob, hasPreferences, isEquippedWith, hasObtained (all four related to the Job Seeker), isOfferingJob (related to the Employer), isRemunerated, isLocated, and requires (related to the JobOffer).  For instance, a Job Offer requiring “Leadership-Initiative” and “DataAnalytics” can be semantically linked to a Job Seeker who possesses the corresponding profile.

An ontology formally represents domain knowledge using classes, subclasses, attributes, and relationships (Debellis, 2021). In OWL, classes define categories (e.g. JobOffer, Person), while subclasses refine them (e.g. Secondary Education, Technical Education, University Education). Object properties (e.g. hasPreferences, is OfferingJob) link individuals and support logical inference (Debellis, 2021). This structure enables flexible, machine-readable representations and logic-based reasoning. Ontologies also support scalability and explainability by separating knowledge from reasoning procedures, allowing updates without modifying inference mechanisms (Marquis et al., 2020). 

By formalising these relationships, the ontology enables AI algorithms to reason over data, facilitating precise and tailored job recommendations. The structured representation allows for both exact and approximate matches, considering hierarchical relations (e.g. Education Level and Qualifications), contextual preferences (e.g. desired work location), and qualitative distinctions (e.g. with/without prior employer recommendations). This ontology-centred approach significantly enhances both the efficiency of candidate-job alignment and the quality of the user experience by enabling personalised, semantically-driven search results. By explicitly modelling key concepts such as skills, qualifications, preferences, and job requirements, the system can move beyond simplistic keyword-matching and instead deliver tailored recommendations that reflect the nuanced profiles of both job seekers and employers. 

Furthermore, the modular and extensible architecture of the ontology facilitates cross-sectoral scalability, ensuring that the model can be adapted to different industries, geographic regions, and evolving labour market trends. This design also supports seamless integration with labour market analytics and external data sources, reinforcing its applicability to contemporary digital employment services.

To validate the semantic structure, the ontology was evaluated using formal reasoning tools available in Protégé and structured, according to OWL 2, DL standards to support formal reasoning. It was tested using Protégé’s built-in reasoners, such as Pellet, and the DL Query tool to verify logical consistency and inferencing capabilities. For example, by creating an individual instance like JobSeekerA and assigning it specific qualifications (e.g. Bachelor’sDegree), skills (Microsoft365Skill), and preferences (DesiredWorkLocation: EU), and then constructing JobOffer instances with corresponding constraints (i.e. requires = Microsoft365Skill, isLocated = EU), the reasoner is able to deduce a match. This form of semantic alignment allows the system to verify subclass hierarchies, property assertions, and compatibility based on logical rules.

Such a structure is crucial for supporting intelligent, AI-enabled job matching. By linking job seeker attributes with job requirements through clearly defined object properties and class restrictions, the ontology enables precise, explainable, and context-aware recommendations. Moreover, its design allows for ongoing evolution. New domain-relevant concepts—such as RemoteWork or SpokenLanguages—and corresponding properties (i.e. SpeaksLanguages) can be incorporated without the need for major restructuring. This capacity for incremental refinement positions the ontology as a robust foundation for building adaptive, intelligent recruitment systems capable of responding to the dynamic nature of the labour market.

As already mentioned, this project has laid the groundwork for an ontology-driven job matching service by formally representing key concepts such as job seekers, job offers, skills, qualifications, and preferences. Through the use of OWL 2 and Protégé, a structured semantic model was designed to enable more accurate and personalised job-matching processes compared to traditional keyword-based systems.

However, several limitations of this prototype must be acknowledged. First, the ontology operates in a closed conceptual space and lacks integration with external semantic resources. Linking the current model to established ontologies such as DBpedia (a project that extracts structured information from Wikipedia), ESCO (a multilingual classification of European skills, competences, qualifications and occupations), or FOAF (a vocabulary for describing people and their relationships) could significantly enrich its vocabulary and reasoning capabilities, enabling a more comprehensive and interoperable system. Second, the current structure is based on predefined assumptions and illustrative categories, which may not fully capture the complexity of real-world labour markets.  

To improve the ontology’s validity and utility, further empirical work is essential. This includes engaging with domain knowledge experts such as human resources professionals, employers, and job seekers to refine the class hierarchies, properties, and competency requirements. Practical feedback from stakeholders would help align the model with actual recruitment processes and candidate experiences.

In addition, ontology-mediated query answering could enable the enrichment of incomplete data sources by leveraging structured background knowledge. This strategy aligns with the vision of the Semantic Web, that it is to say, the web being transformed into a machine-understandable ecosystem of interoperable data (Bernstein et al., 2016; Nasim, 2022).

Consequently, while this ontology provides a promising foundation for AI-based job matching, it remains a conceptual prototype. Its full potential can only be realised through iterative development, real-world validation, and integration with broader knowledge bases and systems.
The full ontology is available at the following URL: http://www.semanticweb.org/user/ontologies/2025/6/AI-job-matching-Ontology


<img width="940" height="496" alt="image" src="https://github.com/user-attachments/assets/b4bf0c29-9fc9-426c-ba85-caf65a00f21e" />



<img width="940" height="1617" alt="image" src="https://github.com/user-attachments/assets/f998c75a-2a4b-4e46-bbcf-eba92a67bcd7" />



[Annex with Protege screenshots]


**References**

Bernstein, A., Hendler, J. and Noy, N.F., 2016. A new look at the semantic web. Communications of the ACM, 59(9), pp.35–37. https://doi.org/10.1145/2890489

Corcho, O. Fernández-López, M. Gómez-Pérez, A. (2007) What Are Ontologies and How Can We Build Them? DOI: 10.4018/978-1-59904-045-5.ch003

Debellis, M. (2021). A Practical Guide to Building OWL Ontologies Using Protégé 5.5 and Plugins

Gruber, T.R., 1993. A translation approach to portable ontology specifications. Knowledge Acquisition, 5(2), pp.199–220. https://doi.org/10.1006/knac.1993.1008

Marquis, P., Papini, O., & Prade, H. (2020). A Guided Tour of Artificial Intelligence Research: Volume I: Knowledge Representation, Reasoning and Learning. Springer

Nasim, T.M., 2022. Improving ontology alignment using machine learning techniques. ProQuest Dissertations & Theses Global



<img width="768" height="514" alt="image" src="https://github.com/user-attachments/assets/9450eb16-5bff-4de2-8ec4-6cbec4577f0b" />




[Back to the top](#knowledge-representation-and-reasoning)



## Individual Reflection





![Capture](https://github.com/user-attachments/assets/1fca64bb-7af3-41bc-99aa-38ceca681bfd)




[Back to the top](#knowledge-representation-and-reasoning)


[Go to main Menu](https://narchondas.github.io/)

